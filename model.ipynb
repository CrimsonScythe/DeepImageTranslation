{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cyclegan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X1L1knLOGQp",
        "outputId": "64315d9a-b7c3-4eff-fab0-61ee1188bf2c"
      },
      "source": [
        "!pip install nibabel\n",
        "!pip install medicaltorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.19.5)\n",
            "Collecting medicaltorch\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/85/b3fa267c6cdec058c937a5046e357de61dda938179aeeca72485f13b1fa2/medicaltorch-0.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (1.19.5)\n",
            "Requirement already satisfied: nibabel>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (3.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (4.41.1)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->medicaltorch) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->medicaltorch) (3.7.4.3)\n",
            "Installing collected packages: medicaltorch\n",
            "Successfully installed medicaltorch-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9untRnc0bKv",
        "cellView": "form"
      },
      "source": [
        "#@title Default title text\n",
        "import numpy as np\n",
        "import numbers\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import torchvision.transforms.functional\n",
        "\n",
        "class MTTransform(object):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        raise NotImplementedError(\"You need to implement the transform() method.\")\n",
        "\n",
        "    def undo_transform(self, sample):\n",
        "        raise NotImplementedError(\"You need to implement the undo_transform() method.\")\n",
        "\n",
        "\n",
        "class UndoCompose(object):\n",
        "    def __init__(self, compose):\n",
        "        self.transforms = compose.transforms\n",
        "\n",
        "    def __call__(self):\n",
        "        for t in self.transforms:\n",
        "            img = t.undo_transform(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "class UndoTransform(object):\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return self.transform.undo_transform(sample)\n",
        "\n",
        "\n",
        "class ToTensor(MTTransform):\n",
        "    \"\"\"Convert a PIL image or numpy array to a PyTorch tensor.\"\"\"\n",
        "\n",
        "    def __init__(self, labeled=True):\n",
        "        self.labeled = labeled\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "\n",
        "        if isinstance(input_data, list):\n",
        "            ret_input = [F.to_tensor(item)\n",
        "                         for item in input_data]\n",
        "        else:\n",
        "            ret_input = F.to_tensor(input_data)\n",
        "\n",
        "        rdict['input'] = ret_input\n",
        "\n",
        "        if self.labeled:\n",
        "            gt_data = sample['gt']\n",
        "            if gt_data is not None:\n",
        "                if isinstance(gt_data, list):\n",
        "                    ret_gt = [F.to_tensor(item)\n",
        "                              for item in gt_data]\n",
        "                else:\n",
        "                    ret_gt = F.to_tensor(gt_data)\n",
        "\n",
        "                rdict['gt'] = ret_gt\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class ToPIL(MTTransform):\n",
        "    def __init__(self, labeled=True):\n",
        "        self.labeled = labeled\n",
        "\n",
        "    def sample_transform(self, sample_data):\n",
        "        # Numpy array\n",
        "        if not isinstance(sample_data, np.ndarray):\n",
        "            input_data_npy = sample_data.numpy()\n",
        "        else:\n",
        "            input_data_npy = sample_data\n",
        "\n",
        "        input_data_npy = np.transpose(input_data_npy, (1, 2, 0))\n",
        "        input_data_npy = np.squeeze(input_data_npy, axis=2)\n",
        "        input_data = Image.fromarray(input_data_npy, mode='F')\n",
        "        return input_data\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "\n",
        "        if isinstance(input_data, list):\n",
        "            ret_input = [self.sample_transform(item)\n",
        "                         for item in input_data]\n",
        "        else:\n",
        "            ret_input = self.sample_transform(input_data)\n",
        "\n",
        "        rdict['input'] = ret_input\n",
        "\n",
        "        if self.labeled:\n",
        "            gt_data = sample['gt']\n",
        "\n",
        "            if isinstance(gt_data, list):\n",
        "                ret_gt = [self.sample_transform(item)\n",
        "                          for item in gt_data]\n",
        "            else:\n",
        "                ret_gt = self.sample_transform(gt_data)\n",
        "\n",
        "            rdict['gt'] = ret_gt\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class res(MTTransform):\n",
        "    \"\"\"Make a center crop of a specified size.\n",
        "\n",
        "    :param segmentation: if it is a segmentation task.\n",
        "                         When this is True (default), the crop\n",
        "                         will also be applied to the ground truth.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, labeled=True):\n",
        "        self.size = size\n",
        "        self.labeled = labeled\n",
        "\n",
        "    @staticmethod\n",
        "    def propagate_params(sample, params):\n",
        "        input_metadata = sample['input_metadata']\n",
        "        input_metadata[\"__centercrop\"] = params\n",
        "        return input_metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(sample):\n",
        "        input_metadata = sample['input_metadata']\n",
        "        return input_metadata[\"__centercrop\"]\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "\n",
        "        w, h = input_data.size\n",
        "        th, tw = self.size\n",
        "        fh = int(round((h - th) / 2.))\n",
        "        fw = int(round((w - tw) / 2.))\n",
        "\n",
        "        params = (fh, fw, w, h)\n",
        "        self.propagate_params(sample, params)\n",
        "\n",
        "        input_data = torchvision.transforms.functional.resize(input_data, self.size)\n",
        "        rdict['input'] = input_data\n",
        "\n",
        "        if self.labeled:\n",
        "            gt_data = sample['gt']\n",
        "            gt_metadata = sample['gt_metadata']\n",
        "            gt_data = torchvision.transforms.functional.resize(gt_data, self.size)\n",
        "            gt_metadata[\"__centercrop\"] = (fh, fw, w, h)\n",
        "            rdict['gt'] = gt_data\n",
        "\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "    def undo_transform(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "        fh, fw, w, h = self.get_params(sample)\n",
        "        th, tw = self.size\n",
        "\n",
        "        pad_left = fw\n",
        "        pad_right = w - pad_left - tw\n",
        "        pad_top = fh\n",
        "        pad_bottom = h - pad_top - th\n",
        "\n",
        "        padding = (pad_left, pad_top, pad_right, pad_bottom)\n",
        "        input_data = F.pad(input_data, padding)\n",
        "        rdict['input'] = input_data\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYDuFIX_Q7Vb"
      },
      "source": [
        "# for custom dataset class\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import glob\n",
        "import nibabel as nib\n",
        "import os\n",
        "from medicaltorch import datasets as mt_datasets\n",
        "from medicaltorch import transforms as mt_transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VFL9saqRMoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e578193-cb59-4ad1-ab01-4e60ce5ce4df"
      },
      "source": [
        "drive.mount('/gdrive')\n",
        "ROOT_DIR = \"/gdrive/My Drive/MM-WHS 2017 Dataset/\"\n",
        "# ROOT_DIR = 'drive/MyDrive/MM-WHS 2017 Dataset/'\n",
        "mri_input_filename = os.path.join(ROOT_DIR,'ct_train',\n",
        "                                          'ct_train_1001_image.nii.gz')\n",
        "mri_gt_filename = os.path.join(ROOT_DIR,'ct_train',\n",
        "                                          'ct_train_1001_label.nii.gz')\n",
        "\n",
        "pair = mt_datasets.SegmentationPair2D(mri_input_filename, mri_gt_filename)\n",
        "slice_pair = pair.get_pair_slice(159)\n",
        "input_slice = slice_pair[\"input\"]\n",
        "gt_slice = slice_pair[\"gt\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ZTOMfiRRz7"
      },
      "source": [
        "print(input_slice.shape)\n",
        "img = input_slice\n",
        "plt.imshow(img,cmap = 'gray')\n",
        "plt.show()\n",
        "img = gt_slice\n",
        "plt.imshow(img,cmap = 'gray')\n",
        "plt.show()\n",
        "\n",
        "img_data,seg_data = pair.get_pair_data()\n",
        "img_data.shape\n",
        "seg_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmPgidEuYaIE"
      },
      "source": [
        "# img_list = os.listdir(os.path.join(ROOT_DIR,'mr_train'))\n",
        "# label_list = os.listdir(os.path.join(ROOT_DIR,'labelsTr'))\n",
        "img_list = sorted(glob.glob(os.path.join(ROOT_DIR, \"mr_train\")+\"/*image.nii.gz\"))\n",
        "label_list = sorted(glob.glob(os.path.join(ROOT_DIR, \"mr_train\")+\"/*label.nii.gz\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVyOhuQIYbrs"
      },
      "source": [
        "# filename_pairs = [(glob.glob(os.path.join(ROOT_DIR, \"mr_train\")+\"/*image.nii.gz\",x),glob.glob(os.path.join(ROOT_DIR, \"mr_train\")+\"/*label.nii.gz\",y)) for x,y in zip(img_list,label_list)]\n",
        "filename_pairs = [(os.path.join(ROOT_DIR,'mr_train',x),os.path.join(ROOT_DIR,'mr_train',y)) for x,y in zip(img_list,label_list)]\n",
        "# filename_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1u2KAs1Fzl8"
      },
      "source": [
        "# img_list = os.listdir(os.path.join(ROOT_DIR,'mr_train'))\n",
        "# label_list = os.listdir(os.path.join(ROOT_DIR,'labelsTr'))\n",
        "img_list2 = sorted(glob.glob(os.path.join(ROOT_DIR, \"ct_train\")+\"/*image.nii.gz\"))\n",
        "label_list2 = sorted(glob.glob(os.path.join(ROOT_DIR, \"ct_train\")+\"/*label.nii.gz\"))\n",
        "\n",
        "filename_pairs2 = [(os.path.join(ROOT_DIR,'ct_train',x),os.path.join(ROOT_DIR,'ct_train',y)) for x,y in zip(img_list2,label_list2)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bkjaCkzZwIH"
      },
      "source": [
        "from torchvision import transforms, utils\n",
        "from skimage import io, transform\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "\n",
        "        # transforms.Resize((256,256)),\n",
        "        # transforms.ToPILImage(),\n",
        "        # transforms.ToTensor()\n",
        "        res((128,128)),\n",
        "        mt_transforms.ToTensor()\n",
        "        \n",
        "        ]\n",
        ")\n",
        "\n",
        "train_dataset = mt_datasets.MRI2DSegmentationDataset(filename_pairs,transform=train_transform)\n",
        "train_dataset2= mt_datasets.MRI2DSegmentationDataset(filename_pairs2,transform=train_transform)\n",
        "\n",
        "type(train_dataset)\n",
        "print(len(train_dataset))\n",
        "\n",
        "# sample of the training dataset\n",
        "train_dataset[0]\n",
        "train_dataset2[0]\n",
        "# print(type(train_dataset[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSOXVsPaIt0"
      },
      "source": [
        "# PyTorch data loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=1,collate_fn=mt_datasets.mt_collate)\n",
        "# batch = next(iter(dataloader))\n",
        "dataloader2= DataLoader(train_dataset2, batch_size=1, collate_fn=mt_datasets.mt_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXuUGpxqtJw2"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.rcParams.update({'font.size': 5})\n",
        "for _ in range(1):\n",
        "    for i, batch in enumerate(zip(dataloader, dataloader2)):\n",
        "      # GAN loss\n",
        "        # fake_B = G_AB(real_A)\n",
        "\n",
        "        # print(fake_B[0].shape)\n",
        "        # print(batch[0][\"input\"][0].shape)\n",
        "        plt.figure(figsize = (1.5,1.5), dpi=128)\n",
        "        plt.imshow(batch[0][\"input\"][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize = (1.5,1.5), dpi=128)\n",
        "        plt.imshow(batch[1][\"input\"][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # plt.imshow(batch[0][\"gt\"][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        # plt.show()\n",
        "\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AExb6bO2br92"
      },
      "source": [
        "# for i,b in enumerate(dataloader):\n",
        "#     print(b['input'].size())\n",
        "#     break\n",
        "\n",
        "for i,b in enumerate(zip(dataloader, dataloader2)):\n",
        "    # print(b[0])\n",
        "    # print('222')\n",
        "    print(b[0][\"input\"].shape)\n",
        "    break\n",
        "    # assert b[0]['input'].size()[2]==256\n",
        "    # assert b[0]['input'].size()[3]==256\n",
        "\n",
        "    # print(b['input'].size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Rz-QyIaUCj"
      },
      "source": [
        "# print(batch['input'].size())\n",
        "# for item in batch['input']:\n",
        "#     plt.imshow(item.squeeze(0),cmap = 'gray')\n",
        "\n",
        "#     plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB4tCWCbs-5g"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "        \n",
        "\n",
        "  \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        channels, height, width = input_shape\n",
        "\n",
        "        # Calculate output shape of image discriminator (PatchGAN)\n",
        "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, 32, normalize=False),\n",
        "            *discriminator_block(32, 64),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(256, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "  \n",
        "class LambdaLR:\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=50):\n",
        "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = element\n",
        "                else:\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1R0T-2DSYR6"
      },
      "source": [
        "'''\n",
        "visualize images\n",
        "'''\n",
        "def vis(image):\n",
        "    plt.imshow(image[0].detach().cpu().permute(1,2,0)[:,:,0],cmap='gray')\n",
        "    plt.show()\n",
        "  # for _ in range(10):\n",
        "      # for i, batch in enumerate(zip(dataloader, dataloader2)):\n",
        "        # GAN loss\n",
        "          # fake_B = G_AB(real_A)\n",
        "\n",
        "          # print(fake_B[0].shape)\n",
        "          \n",
        "          # plt.imshow(fake_B[0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "          # plt.show()\n",
        "\n",
        "          # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATdyNeDuYdFm"
      },
      "source": [
        "# https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/21\n",
        "# https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/random_ops.py#L163\n",
        "# def parameterized_truncated_normal(uniform, mu=0.0, sigma=1.0, a, b):\n",
        "#     normal = torch.distributions.normal.Normal(0, 1)\n",
        "\n",
        "#     alpha = (a - mu) / sigma\n",
        "#     beta = (b - mu) / sigma\n",
        "\n",
        "#     alpha_normal_cdf = normal.cdf(alpha)\n",
        "#     p = alpha_normal_cdf + (normal.cdf(beta) - alpha_normal_cdf) * uniform\n",
        "\n",
        "#     p = p.numpy()\n",
        "#     one = np.array(1, dtype=p.dtype)\n",
        "#     epsilon = np.array(np.finfo(p.dtype).eps, dtype=p.dtype)\n",
        "#     v = np.clip(2 * p - 1, -one + epsilon, one - epsilon)res\n",
        "#     x = mu + sigma * np.sqrt(2) * torch.erfinv(torch.from_numpy(v))\n",
        "#     x = torch.clamp(x, a, b)\n",
        "\n",
        "#     return x\n",
        "\n",
        "\n",
        "# def truncated_normal(uniform):\n",
        "#     return parameterized_truncated_normal(uniform, mu=0.0, sigma=1.0, a=-2, b=2)\n",
        "def truncated_normal(t, mean=0.0, std=0.01):\n",
        "    torch.nn.init.normal_(t, mean=mean, std=std)\n",
        "    while True:\n",
        "      cond = torch.logical_or(t < mean - 2*std, t > mean + 2*std)\n",
        "      if not torch.sum(cond):\n",
        "        break\n",
        "      t = torch.where(cond.cuda(), torch.nn.init.normal_(torch.ones(t.shape), mean=mean, std=std).cuda(), t.cuda()).cuda()\n",
        "      \n",
        "    # return t\n",
        "\n",
        "def compute_conv_output(H, K, S):\n",
        "  return int(((H-K+2*0)/S)+1)\n",
        "\n",
        "def compute_padding(out, H, K, S):\n",
        "  return int((S*(out-1)-H+K)/2)\n",
        "\n",
        "def compute_padding_deconv(out, H, K, S):\n",
        "  return int((-out+(H-1)*S+K)/2)\n",
        "\n",
        "def compute_maxpool_padding(S, H, f):\n",
        "  '''\n",
        "  H2=(H1-f)/s+1\n",
        "  '''\n",
        "  return int((S*(H-1)-H+f)/2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nswSWccwQI-5"
      },
      "source": [
        "\n",
        "class General_Conv2D(nn.Module):\n",
        "    # @torch.no_grad()\n",
        "    def init_weights(self, n):\n",
        "      if type(n) == nn.Conv2d:\n",
        "          truncated_normal(n.weight, 0.01)\n",
        "          \n",
        "\n",
        "    def __init__(self, in_features=0, out_features=64, k=7, s=1, stddev=0.01, do_relu=True, keep_rate=0.75, relu_factor=0, norm_type=None, train=True, padding=False, input_dim=0):\n",
        "        super(General_Conv2D, self).__init__()\n",
        "        self.std=stddev\n",
        "        self.keep_rate=keep_rate\n",
        "        self.relu_factor=relu_factor\n",
        "        self.norm_type=norm_type\n",
        "        self.do_relu=do_relu\n",
        "        self.k=k\n",
        "        self.s=s\n",
        "        self.padding=padding\n",
        "\n",
        "        self.dropout = nn.Dropout(p=keep_rate)\n",
        "        \n",
        "        if norm_type=='Batch':\n",
        "          if train:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True).train()\n",
        "          else:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True)\n",
        "        elif norm_type=='Ins':\n",
        "          if train:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features).train()\n",
        "          else:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features)\n",
        "        \n",
        "        self.relu=nn.ReLU()\n",
        "        self.lrealu=nn.LeakyReLU()\n",
        "        \n",
        "        self.w= nn.Parameter(torch.randn(out_features, in_features, k,k))\n",
        "        truncated_normal(self.w)\n",
        "        self.b=nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        initialize weights\n",
        "        '''\n",
        "        \n",
        "        if self.padding==False:\n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=0)\n",
        "\n",
        "        else:\n",
        "          \n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=compute_padding(x.shape[-1], x.shape[-1], self.k, self.s))\n",
        "        \n",
        "        '''\n",
        "        dropout\n",
        "        '''\n",
        "        if not self.keep_rate is None:\n",
        "          x=self.dropout(x)\n",
        "\n",
        "        '''\n",
        "        norm\n",
        "        '''\n",
        "        if (self.norm_type=='Batch'):\n",
        "          x=self.batchnorm(x)\n",
        "        else:# Instance norm\n",
        "          x=self.instancenorm(x)\n",
        "\n",
        "        '''\n",
        "        act fnc\n",
        "        '''\n",
        "        if (self.do_relu==True):\n",
        "          if (self.relu_factor==0):\n",
        "            x=self.relu(x)\n",
        "          else:\n",
        "            x=self.lrealu(x)\n",
        "        \n",
        "        '''\n",
        "        update global dim\n",
        "        '''\n",
        "        global dim\n",
        "        dim=x.shape[-1]\n",
        "\n",
        "        return x\n",
        "\n",
        "        # return x + self.block(x)\n",
        "\n",
        "\n",
        "class Resnet_Block(nn.Module):\n",
        "    def __init__(self, in_features, out_features , padding=\"REFLECT\", norm_type=None, keep_rate=0.75):\n",
        "        super(Resnet_Block, self).__init__()\n",
        "\n",
        "        if padding=='REFLECT':\n",
        "          pad_layer=nn.ReflectionPad2d(1)\n",
        "        elif padding=='CONSTANT':\n",
        "          pad_layer=nn.ZeroPad2d(1)\n",
        "        else: # SYMMETRIC\n",
        "          pad_layer=nn.ReplicationPad2d(1)\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            pad_layer,\n",
        "            General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, norm_type=norm_type, keep_rate=keep_rate, padding=False),\n",
        "            pad_layer,\n",
        "            General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, do_relu=False, norm_type=norm_type, keep_rate=keep_rate, padding=False)\n",
        "        )\n",
        "\n",
        "        # self.l1=nn.ReflectionPad2d(1)\n",
        "        # self.l2=General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, norm_type=norm_type, keep_rate=keep_rate, padding='VALID')\n",
        "        # self.l3=nn.ReflectionPad2d(1)\n",
        "        # self.l4=General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, do_relu=False, norm_type=norm_type, keep_rate=keep_rate, padding='VALID')\n",
        "\n",
        "        self.relu=nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        # x=self.l1(x)\n",
        "        # print(x.shape)\n",
        "        # x=self.l2(x)\n",
        "        # print(x.shape)\n",
        "        # x=self.l3(x)\n",
        "        # print(x.shape)\n",
        "        # x=self.l4(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # print(x.shape)\n",
        "        out = self.block(x)\n",
        "        # print(out.shape)\n",
        "\n",
        "        return self.relu(out+x)\n",
        "\n",
        "class Resnet_Block_ds(nn.Module):\n",
        "    def __init__(self, in_features=0, out_features=0, padding=\"REFLECT\",dim=0, norm_type=None, keep_rate=0.75):\n",
        "        super(Resnet_Block_ds, self).__init__()\n",
        "\n",
        "        self.in_features=in_features\n",
        "        self.out_features=out_features\n",
        "\n",
        "        if padding=='REFLECT':\n",
        "          pad_layer=nn.ReflectionPad2d(1)\n",
        "        elif padding=='CONSTANT':\n",
        "          pad_layer=nn.ZeroPad2d(1)\n",
        "        else: # SYMMETRIC\n",
        "          pad_layer=nn.ReplicationPad2d(1)\n",
        "\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            \n",
        "            pad_layer,\n",
        "            General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, norm_type=norm_type, keep_rate=keep_rate, padding=False),\n",
        "            pad_layer,\n",
        "            General_Conv2D(out_features, out_features, k=3, s=1, stddev=0.01, do_relu=False, norm_type=norm_type, keep_rate=keep_rate, padding=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "     \n",
        "        out = self.block(x)\n",
        "        '''\n",
        "        pad channel dim\n",
        "        '''\n",
        "        pd=(self.out_features-self.in_features) // 2\n",
        "\n",
        "        padding = torch.zeros(x.shape[0], pd, x.shape[2], x.shape[3]).cuda()\n",
        "        padded_inp = torch.cat((x, padding), 1)\n",
        "        padded_inp = torch.cat((padding, padded_inp), 1)\n",
        "\n",
        "\n",
        "        return torch.nn.functional.relu(out+padded_inp)\n",
        "  \n",
        "class drn_Block(nn.Module):\n",
        "    def __init__(self, in_features, keep_rate=0.75):\n",
        "        super(drn_Block, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(2),\n",
        "            nn.Conv2d(in_features, in_features, 3, dilation=2),\n",
        "            nn.Dropout(0.75),\n",
        "            nn.BatchNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(2),\n",
        "            nn.Conv2d(in_features, in_features, 3, dilation=2),\n",
        "            nn.Dropout(0.75),\n",
        "            nn.BatchNorm2d(in_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.block(x)\n",
        "        \n",
        "\n",
        "        return torch.nn.functional.relu(x)\n",
        "\n",
        "\n",
        "class General_Conv2D_GA(nn.Module):\n",
        "    # @torch.no_grad()\n",
        "    def init_weights(self, n):\n",
        "      if type(n) == nn.Conv2d:\n",
        "          truncated_normal(n.weight, 0.02)\n",
        "          \n",
        "\n",
        "    def __init__(self, in_features=0, out_features=64, k=7, s=1, stddev=0.02, do_relu=True, keep_rate=0.75, relu_factor=0, norm_type=None, train=True, padding=False, input_dim=0):\n",
        "        super(General_Conv2D_GA, self).__init__()\n",
        "        self.std=stddev\n",
        "        self.keep_rate=keep_rate\n",
        "        self.relu_factor=relu_factor\n",
        "        self.norm_type=norm_type\n",
        "        self.do_relu=do_relu\n",
        "        self.k=k\n",
        "        self.s=s\n",
        "        self.padding=padding\n",
        "\n",
        "        self.dropout = nn.Dropout(p=keep_rate)\n",
        "        \n",
        "        if norm_type=='Batch':\n",
        "          if train:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True).train()\n",
        "          else:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True)\n",
        "        elif norm_type=='Ins':\n",
        "          if train:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features).train()\n",
        "          else:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features)\n",
        "        \n",
        "        self.relu=nn.ReLU()\n",
        "        self.lrealu=nn.LeakyReLU()\n",
        "        \n",
        "        self.w= nn.Parameter(torch.randn(out_features, in_features, k,k))\n",
        "        truncated_normal(self.w)\n",
        "        self.b=nn.Parameter(torch.randn(out_features))\n",
        "        torch.nn.init.constant_(self.b, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        initialize weights\n",
        "        '''\n",
        "        \n",
        "        print('GA')\n",
        "        print(x.shape)\n",
        "\n",
        "        if self.padding==False:\n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=0)\n",
        "\n",
        "        else:\n",
        "          \n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=compute_padding(x.shape[-1], x.shape[-1], self.k, self.s))\n",
        "        \n",
        "        '''\n",
        "        dropout\n",
        "        '''\n",
        "        if not self.keep_rate is None:\n",
        "          x=self.dropout(x)\n",
        "\n",
        "        '''\n",
        "        norm\n",
        "        '''\n",
        "        if (self.norm_type=='Batch'):\n",
        "          x=self.batchnorm(x)\n",
        "        elif self.norm_type=='Ins':# Instance norm\n",
        "          x=self.instancenorm(x)\n",
        "\n",
        "        '''\n",
        "        act fnc\n",
        "        '''\n",
        "        if (self.do_relu==True):\n",
        "          if (self.relu_factor==0):\n",
        "            x=self.relu(x)\n",
        "          else:\n",
        "            x=self.lrealu(x)\n",
        "        \n",
        "       \n",
        "\n",
        "        return x\n",
        "\n",
        "        # return x + self.block(x)\n",
        "\n",
        "\n",
        "class Resnet_Block_ins(nn.Module):\n",
        "    def __init__(self, in_features=0, out_features=0, padding=\"REFLECT\",dim=0, norm_type=None, keep_rate=0.75):\n",
        "        super(Resnet_Block_ins, self).__init__()\n",
        "\n",
        "        self.in_features=in_features\n",
        "        self.out_features=out_features\n",
        "\n",
        "        if padding=='REFLECT':\n",
        "          pad_layer=nn.ReflectionPad2d(1)\n",
        "        elif padding=='CONSTANT':\n",
        "          pad_layer=nn.ZeroPad2d(1)\n",
        "        else: # SYMMETRIC\n",
        "          pad_layer=nn.ReplicationPad2d(1)\n",
        "\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            \n",
        "            pad_layer,\n",
        "            General_Conv2D_GA(in_features, out_features, k=3, s=1, stddev=0.02, norm_type='Ins', keep_rate=keep_rate, padding=False, do_relu=True),\n",
        "            pad_layer,\n",
        "            General_Conv2D_GA(out_features, out_features, k=3, s=1, stddev=0.02, norm_type='Ins', keep_rate=keep_rate, padding=False, do_relu=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "     \n",
        "        out = self.block(x)\n",
        "        \n",
        "        return torch.nn.functional.relu(out+x)\n",
        "\n",
        "class General_Deconv2D(nn.Module):\n",
        "    # @torch.no_grad()\n",
        "    def init_weights(self, n):\n",
        "      if type(n) == nn.Conv2d:\n",
        "          truncated_normal(n.weight, 0.02)\n",
        "          \n",
        "\n",
        "    def __init__(self, in_features=0, out_features=64, k=7, s=1, stddev=0.02, do_relu=True, keep_rate=0.75, relu_factor=0, norm_type=None, train=True, padding=False, input_dim=0, out_shape=0):\n",
        "        super(General_Deconv2D, self).__init__()\n",
        "        self.std=stddev\n",
        "        self.keep_rate=keep_rate\n",
        "        self.relu_factor=relu_factor\n",
        "        self.norm_type=norm_type\n",
        "        self.do_relu=do_relu\n",
        "        self.k=k\n",
        "        self.s=s\n",
        "        self.padding=padding\n",
        "        self.out_shape=out_shape\n",
        "        \n",
        "        if norm_type=='Batch':\n",
        "          if train:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True).train()\n",
        "          else:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True)\n",
        "        elif norm_type=='Ins':\n",
        "          if train:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features).train()\n",
        "          else:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features)\n",
        "        \n",
        "        self.relu=nn.ReLU()\n",
        "        self.lrealu=nn.LeakyReLU()\n",
        "        \n",
        "        self.w= nn.Parameter(torch.randn(in_features, out_features, k,k))\n",
        "        truncated_normal(self.w)\n",
        "        self.b=nn.Parameter(torch.randn(out_features))\n",
        "        torch.nn.init.constant_(self.b, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        initialize weights\n",
        "        '''\n",
        "\n",
        "        print('deconv')\n",
        "        print(x.shape)\n",
        "\n",
        "        if self.padding==False:\n",
        "          x = torch.nn.functional.conv_transpose2d(x, self.w, bias=self.b, stride=self.s, padding=0)\n",
        "          # x = torch.nn.functional.conv2d(x, self.w, self.b, padding=0)\n",
        "\n",
        "        else:\n",
        "          x = torch.nn.functional.conv_transpose2d(x, self.w, bias=self.b, stride=self.s, padding=compute_padding_deconv(self.out_shape, x.shape[-1], self.k, self.s))\n",
        "          # x = torch.nn.functional.conv2d(x, self.w, self.b, padding=compute_padding(x.shape[-1], x.shape[-1], self.k, self.s))\n",
        "        \n",
        "\n",
        "        '''\n",
        "        norm\n",
        "        '''\n",
        "        if (self.norm_type=='Batch'):\n",
        "          x=self.batchnorm(x)\n",
        "        else:# Instance norm\n",
        "          x=self.instancenorm(x)\n",
        "\n",
        "        '''\n",
        "        act fnc\n",
        "        '''\n",
        "        if (self.do_relu==True):\n",
        "          if (self.relu_factor==0):\n",
        "            x=self.relu(x)\n",
        "          else:\n",
        "            x=self.lrealu(x)\n",
        "\n",
        "\n",
        "        'explicit resize'\n",
        "        # x=torch.reshape(x, (x.shape[0], x.shape[1], x.shape[2]-1, x.shape[3]-1))\n",
        "        x=x[:,:,:self.out_shape,:self.out_shape]\n",
        "        \n",
        "\n",
        "        return x\n",
        "\n",
        "        # return x + self.block(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVPIOpijFi2C"
      },
      "source": [
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, skip=True):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        self.skip=skip\n",
        "        # self.input_img=input_img\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ZeroPad2d(3),\n",
        "            General_Conv2D_GA(in_features=1, out_features=32, k=7, s=1, stddev=0.02, do_relu=True, norm_type='Ins', train=True, padding=False),\n",
        "            General_Conv2D_GA(in_features=32, out_features=32*2, k=3, s=2, stddev=0.02, do_relu=True, norm_type='Ins', train=True, padding=True),\n",
        "            General_Conv2D_GA(in_features=32*2, out_features=32*4, k=3, s=2, stddev=0.02, do_relu=True, norm_type='Ins', train=True, padding=True),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=32*4, out_features=32*4, padding='CONSTANT'),\n",
        "            General_Deconv2D(in_features=32*4, out_features=32*2, k=3, s=2, stddev=0.02, norm_type='Ins', out_shape=64, padding=True),\n",
        "            General_Deconv2D(in_features=32*2, out_features=32, k=3, s=2, stddev=0.02, norm_type='Ins', out_shape=128, padding=True),\n",
        "            General_Conv2D_GA(in_features=32, out_features=1, k=7, s=1, stddev=0.02, do_relu=False, train=True, padding=True),\n",
        "            )\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "      out=self.block(x)\n",
        "      if self.skip==True:\n",
        "        \n",
        "        return torch.nn.functional.tanh(x+out)\n",
        "      else:\n",
        "        return torch.nn.functional.tanh(out)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2wCespUOIZj"
      },
      "source": [
        "'''\n",
        "input shape: B,C,H,W\n",
        "'''\n",
        "class EncoderNet(nn.Module):\n",
        "    def __init__(self, train, input_shape):\n",
        "        super(EncoderNet, self).__init__()\n",
        "        self.train=train\n",
        "        self.input_shape=input_shape\n",
        "\n",
        "  \n",
        "\n",
        "        self.out_c1 = General_Conv2D(in_features=1, out_features=16, padding=True, train=self.train, input_dim=self.input_shape[-1], norm_type='Batch')\n",
        "        self.out_res1 = Resnet_Block(in_features=16,out_features=16,norm_type='Batch')\n",
        "        self.out1 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=input_shape[-1], f=1))\n",
        "        \n",
        "        self.out_res2 = Resnet_Block_ds(in_features=16, out_features=16*2, norm_type='Batch', padding='CONSTANT')\n",
        "        # self.out2 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=input_shape[-1], f=1))\n",
        "\n",
        "        self.out_res3 = Resnet_Block_ds(in_features=16*2, out_features=16*4, norm_type='Batch', padding='CONSTANT')\n",
        "        self.out_res4 = Resnet_Block(in_features=16*4,out_features=16*4,norm_type='Batch', padding='CONSTANT')\n",
        "        # self.out3 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=input_shape[-1], f=1))\n",
        "\n",
        "        self.out_res5 = Resnet_Block_ds(in_features=16*4,out_features=16*8,norm_type='Batch', padding='CONSTANT')\n",
        "        self.out_res6 = Resnet_Block(in_features=16*8,out_features=16*8,norm_type='Batch', padding='CONSTANT')\n",
        "\n",
        "        self.out_res7 = Resnet_Block_ds(in_features=16*8,out_features=16*16,norm_type='Batch', padding='CONSTANT')\n",
        "        self.out_res8 = Resnet_Block(in_features=16*16,out_features=16*16,norm_type='Batch', padding='CONSTANT')\n",
        "\n",
        "        self.out_res9 = Resnet_Block(in_features=16*16,out_features=16*16,norm_type='Batch', padding='CONSTANT')\n",
        "        self.out_res10= Resnet_Block(in_features=16*16,out_features=16*16,norm_type='Batch', padding='CONSTANT')\n",
        "\n",
        "        self.out_res11= Resnet_Block_ds(in_features=16*16,out_features=16*32,norm_type='Batch', padding='CONSTANT')\n",
        "        self.out_res12= Resnet_Block(in_features=16*32,out_features=16*32,norm_type='Batch', padding='CONSTANT')\n",
        "\n",
        "        self.out_drn1 = drn_Block(in_features=16*32)\n",
        "        self.out_drn2 = drn_Block(in_features=16*32)\n",
        "        '''\n",
        "        use global dim below !!\n",
        "        '''\n",
        "        self.out_c2 = General_Conv2D(in_features=16*32, out_features=16*32, k=3, padding=True, train=self.train, input_dim=0, norm_type='Batch')\n",
        "        self.out_c3 = General_Conv2D(in_features=16*32, out_features=16*32, k=3, padding=True, train=self.train, input_dim=0, norm_type='Batch')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = General_Conv2D(in_features=3, out_features=16, padding='SAME', train=self.train, input_dim=self.input_dim, norm_type='Batch')(x)\n",
        "        # x = Resnet_Block(in_features=x.shape[1],out_features=out,norm_type='BATCH')(x)\n",
        "        '''\n",
        "        separate kernel size implementation from tf not available in pytorch\n",
        "        TODO\n",
        "        '''\n",
        "  #       def compute_maxpool_padding(S, H, f):\n",
        "  # return (S*(H-1)-H+f)/2\n",
        "        \n",
        "        # x = nn.MaxPool2D(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=x.shape[-1], f=1))\n",
        "\n",
        "        # x = Resnet_Block_ds(in_features=x.shape[1], out_features=16*2, dim=16, norm_type='Batch', padding='CONSTANT')(X)\n",
        "        x=self.out_c1(x)\n",
        "        x=self.out_res1(x)\n",
        "        x=self.out1(x)\n",
        "        x=self.out_res2(x)\n",
        "        self.out2 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=x.shape[-1], f=1))\n",
        "        x=self.out2(x)\n",
        "        x=self.out_res3(x)\n",
        "        x=self.out_res4(x)\n",
        "        self.out3 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=x.shape[-1], f=1))\n",
        "        x=self.out3(x)\n",
        "        x=self.out_res5(x)\n",
        "        x=self.out_res6(x)\n",
        "        x=self.out_res7(x)\n",
        "        x=self.out_res8(x)\n",
        "        x=self.out_res9(x)\n",
        "        x=self.out_res10(x)\n",
        "       \n",
        "        x=self.out_res11(x)\n",
        "        x=self.out_res12(x)\n",
        "        y=x\n",
        "        x=self.out_drn1(x)\n",
        "        x=self.out_drn2(x)\n",
        "        '''\n",
        "        '''\n",
        "        x=self.out_c2(x)\n",
        "        x=self.out_c3(x)\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKN5BOs7OTac"
      },
      "source": [
        "# class DecoderNet(nn.Module):\n",
        "# class DecoderNet(nn.Module):\n",
        "#     def __init__(self, input_shape, num_residual_blocks):\n",
        "#         super(GeneratorResNet, self).__init__()\n",
        "\n",
        "#         channels = input_shape[0]\n",
        "\n",
        "#         # Initial convolution block\n",
        "#         # out_features = 64\n",
        "#         out_features=32\n",
        "#         model = [\n",
        "#             nn.ReflectionPad2d(3),\n",
        "#             nn.Conv2d(channels, out_features, 7),\n",
        "#             nn.InstanceNorm2d(out_features),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#         ]\n",
        "#         in_features = out_features\n",
        "\n",
        "#         # Downsampling\n",
        "#         for _ in range(2):\n",
        "#             out_features *= 2\n",
        "#             model += [\n",
        "#                 nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "#                 nn.InstanceNorm2d(out_features),\n",
        "#                 nn.ReLU(inplace=True),\n",
        "#             ]\n",
        "#             in_features = out_features\n",
        "\n",
        "#         # Residual blocks\n",
        "#         for _ in range(num_residual_blocks):\n",
        "#             model += [ResidualBlock(out_features)]\n",
        "\n",
        "#         # Upsampling\n",
        "#         for _ in range(2):\n",
        "#             out_features //= 2\n",
        "#             model += [\n",
        "#                 nn.Upsample(scale_factor=2),\n",
        "#                 nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
        "#                 nn.InstanceNorm2d(out_features),\n",
        "#                 nn.ReLU(inplace=True),\n",
        "#             ]\n",
        "#             in_features = out_features\n",
        "\n",
        "#         # Output layer\n",
        "#         model += [nn.ReflectionPad2d(3), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
        "\n",
        "#         self.model = nn.Sequential(*model)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKIcdHO3OHc"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "# from utils import *\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import sys\n",
        "import random\n",
        "\n",
        "# Losses\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "\n",
        "# G_AB = GeneratorResNet((1, 128, 128), 9)\n",
        "# G_BA = GeneratorResNet((1, 128, 128), 9)\n",
        "D_A = Discriminator((1, 128, 128))\n",
        "D_B = Discriminator((1, 128, 128))\n",
        "\n",
        "'''\n",
        "Encoder\n",
        "'''\n",
        "# Encoder = EncoderNet(train=True, input_shape=(2,1,128,128))\n",
        "'''\n",
        "Decoder\n",
        "'''\n",
        "# Decoder = DecoderNet()\n",
        "\n",
        "if cuda:\n",
        "    # Encoder = Encoder.cuda()\n",
        "    # G_AB = G_AB.cuda()\n",
        "    # G_BA = G_BA.cuda()\n",
        "    D_A = D_A.cuda()\n",
        "    D_B = D_B.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_cycle.cuda()\n",
        "    criterion_identity.cuda()\n",
        "\n",
        "opt={\"lr\":0.0002,\"b1\":0.5, \"b2\":0.999, \"epoch\":0, \"n_epochs\":10, \"decay_epoch\":5, \"lambda_cyc\":10.0, \"lambda_id\":5.0}\n",
        "\n",
        "# # Optimizers\n",
        "# optimizer_G = torch.optim.Adam(\n",
        "#     itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"])\n",
        "# )\n",
        "# optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "# optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "\n",
        "# # Learning rate update schedulers\n",
        "# lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "#     optimizer_G, lr_lambda=LambdaLR(opt[\"n_epochs\"], opt[\"epoch\"], opt[\"decay_epoch\"]).step\n",
        "# )\n",
        "# lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
        "#     optimizer_D_A, lr_lambda=LambdaLR(opt[\"n_epochs\"], opt[\"epoch\"], opt[\"decay_epoch\"]).step\n",
        "# )\n",
        "# lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
        "#     optimizer_D_B, lr_lambda=LambdaLR(opt[\"n_epochs\"], opt[\"epoch\"], opt[\"decay_epoch\"]).step\n",
        "# )\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "# fake_A_buffer = ReplayBuffer()\n",
        "# fake_B_buffer = ReplayBuffer()\n",
        "\n",
        "# def sample_images(batches_done):\n",
        "#     \"\"\"Saves a generated sample from the test set\"\"\"\n",
        "#     imgs = next(iter(val_dataloader))\n",
        "#     G_AB.eval()\n",
        "#     G_BA.eval()\n",
        "#     real_A = Variable(imgs[\"A\"].type(Tensor))\n",
        "#     fake_B = G_AB(real_A)\n",
        "#     real_B = Variable(imgs[\"B\"].type(Tensor))\n",
        "#     fake_A = G_BA(real_B)\n",
        "#     # Arange images along x-axis\n",
        "#     real_A = make_grid(real_A, nrow=5, normalize=True)\n",
        "#     real_B = make_grid(real_B, nrow=5, normalize=True)\n",
        "#     fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
        "#     fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
        "#     # Arange images along y-axis\n",
        "#     image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
        "#     save_image(image_grid, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), normalize=False)\n",
        "\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "prev_time = time.time()\n",
        "for epoch in range(opt[\"epoch\"], opt[\"n_epochs\"]):\n",
        "    for i, batch in enumerate(zip(dataloader, dataloader2)):\n",
        "\n",
        "        # Set model input\n",
        "        real_A = Variable(batch[0][\"input\"].type(Tensor)) # mri\n",
        "        real_B = Variable(batch[1][\"input\"].type(Tensor)) # ct\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "        '''\n",
        "        test forward pass\n",
        "        '''\n",
        "        G_A=GeneratorResNet(skip=True)\n",
        "        G_A=G_A.cuda()\n",
        "\n",
        "        fake_imgs_b = G_A(real_A)\n",
        "\n",
        "\n",
        "\n",
        "        latent_fake_b, latent_fake_b_2 =Encoder(real_B)\n",
        "        \n",
        "\n",
        "        G_AB.train()\n",
        "        G_BA.train()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        \n",
        "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "\n",
        "        # vis(real_A)\n",
        "        # vis(G_AB(real_A))\n",
        "\n",
        "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = G_AB(real_A)\n",
        "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
        "        fake_A = G_BA(real_B)\n",
        "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "\n",
        "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "        # Cycle loss\n",
        "        '''\n",
        "        Here we are swapping out this particular instance of G_BA with an Encoder and Decoder\n",
        "        '''\n",
        "        # recov_A = G_BA(fake_B)\n",
        "        # latent_fake_B, _ = Encoder(fake_B)\n",
        "        # recov_A = Decoder(latent_fake_B, fake_B)\n",
        "\n",
        "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "        recov_B = G_AB(fake_A)\n",
        "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "\n",
        "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_GAN + opt[\"lambda_cyc\"] * loss_cycle + opt[\"lambda_id\"] * loss_identity\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator A\n",
        "        # -----------------------\n",
        "\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
        "        # Fake loss (on batch of previously generated samples)\n",
        "        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
        "        # Total loss\n",
        "        loss_D_A = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator B\n",
        "        # -----------------------\n",
        "\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
        "        # Fake loss (on batch of previously generated samples)\n",
        "        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
        "        # Total loss\n",
        "        loss_D_B = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        loss_D = (loss_D_A + loss_D_B) / 2\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "\n",
        "        # Determine approximate time left\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        batches_left = opt[\"n_epochs\"] * len(dataloader) - batches_done\n",
        "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # Print log\n",
        "        sys.stdout.write(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
        "            % (\n",
        "                epoch,\n",
        "                opt[\"n_epochs\"],\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D.item(),\n",
        "                loss_G.item(),\n",
        "                loss_GAN.item(),\n",
        "                loss_cycle.item(),\n",
        "                loss_identity.item(),\n",
        "                time_left,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # # If at sample interval save image\n",
        "        # if batches_done % opt.sample_interval == 0:\n",
        "        #     sample_images(batches_done)\n",
        "\n",
        "    # Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_A.step()\n",
        "    lr_scheduler_D_B.step()\n",
        "\n",
        "    # if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
        "    #     # Save model checkpoints\n",
        "    #     torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (opt.dataset_name, epoch))\n",
        "    #     torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (opt.dataset_name, epoch))\n",
        "    #     torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (opt.dataset_name, epoch))\n",
        "    #     torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (opt.dataset_name, epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of2afO6AyDsV"
      },
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "\n",
        "# x = tf.Variable(np.empty((2, 16, 120,120), dtype=np.float32))\n",
        "# y = tf.Variable(np.empty((2, 16, 120,120), dtype=np.float32))\n",
        "# print((x+y).shape)\n",
        "\n",
        "# a = torch.empty((2,16,120,120))\n",
        "# b = torch.empty((2,16,120,120))\n",
        "# print((a+b).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53naIDxtf00l"
      },
      "source": [
        "# vis(G_AB(real_B))\n",
        "# vis(G_AB(real_A))\n",
        "# vis(real_B)\n",
        "\n",
        "# vis(real_B)\n",
        "# vis(G_BA(real_B))\n",
        "\n",
        "\n",
        "for _ in range(1):\n",
        "    for i, batch in enumerate(zip(dataloader, dataloader2)):\n",
        "        vis(G_AB(real_B))\n",
        "        vis(real_B)\n",
        "\n",
        "        vis(G_BA(real_A))\n",
        "        vis(real_A)\n",
        "        \n",
        "      # GAN loss\n",
        "        # fake_B = G_AB(real_A)\n",
        "\n",
        "        # print(fake_B[0].shape)\n",
        "        \n",
        "        # plt.imshow(fake_B[0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        # plt.show()\n",
        "\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQTMUU-x-u1T"
      },
      "source": [
        "# # test forward pass\n",
        "# G_AB = GeneratorResNet((1, 256, 256), 9)\n",
        "# G_BA = GeneratorResNet((1, 256, 256), 9)\n",
        "# G_AB.train()\n",
        "# G_BA.train()\n",
        "# optimizer_G = torch.optim.Adam(\n",
        "#     itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999)\n",
        "# )\n",
        "# # for epoch in range(1):\n",
        "# for i, batch in enumerate(dataloader):\n",
        "#   G_AB(batch[\"input\"])\n",
        "    \n",
        "'''class mrctDataset(Dataset):\n",
        "    \"\"\"MR CT dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \n",
        "        self.files_A = glob.glob(os.path.join(root_dir, \"ct_train\")+\"/*image.nii\")\n",
        "        self.files_B = glob.glob(os.path.join(root_dir, \"mri_train\")+\"/*image.nii\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return (max(len(self.files_A), len(self.files_B)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      nii_image_A = nib.load(self.files_A[idx])\n",
        "      nii_image_B = nib.load(self.files_B[idx])\n",
        "\n",
        "      return {'A':nii_image_A, 'B':nii_image_B}'''\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP0nISFM_uri"
      },
      "source": [
        "'''mrct_dataset = mrctDataset(root_dir='/gdrive/My Drive/data/')\n",
        "print(mrct_dataset[0]['A'])'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NSDja9JmH-F"
      },
      "source": [
        "# for i,b in enumerate(dataloader):\n",
        "#     print(b['input'].size())\n",
        "#     break\n",
        "\n",
        "for i,b in enumerate(zip(dataloader, dataloader2)):\n",
        "    # print(b[0])\n",
        "    # print('222')\n",
        "    # print(b[0][\"input\"].shape)\n",
        "    vis(b[0][\"input\"])\n",
        "    vis(b[1][\"input\"])\n",
        "    break\n",
        "    # assert b[0]['input'].size()[2]==256\n",
        "    # assert b[0]['input'].size()[3]==256\n",
        "\n",
        "    # print(b['input'].size())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}