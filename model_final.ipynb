{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X1L1knLOGQp",
        "outputId": "03a24273-1f3a-4fbc-da97-0d7a99179035"
      },
      "source": [
        "!pip install nibabel\n",
        "!pip install medicaltorch\n",
        "# !pip install visdom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.19.5)\n",
            "Collecting medicaltorch\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/85/b3fa267c6cdec058c937a5046e357de61dda938179aeeca72485f13b1fa2/medicaltorch-0.2-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (1.19.5)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: nibabel>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (3.0.2)\n",
            "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (4.41.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from medicaltorch) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->medicaltorch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->medicaltorch) (7.1.2)\n",
            "Installing collected packages: medicaltorch\n",
            "Successfully installed medicaltorch-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9untRnc0bKv",
        "cellView": "form"
      },
      "source": [
        "#@title [Please run] Custom transformations \n",
        "import numpy as np\n",
        "import numbers\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import torchvision.transforms.functional\n",
        "\n",
        "class MTTransform(object):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        raise NotImplementedError(\"You need to implement the transform() method.\")\n",
        "\n",
        "    def undo_transform(self, sample):\n",
        "        raise NotImplementedError(\"You need to implement the undo_transform() method.\")\n",
        "\n",
        "\n",
        "class UndoCompose(object):\n",
        "    def __init__(self, compose):\n",
        "        self.transforms = compose.transforms\n",
        "\n",
        "    def __call__(self):\n",
        "        for t in self.transforms:\n",
        "            img = t.undo_transform(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "class UndoTransform(object):\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return self.transform.undo_transform(sample)\n",
        "\n",
        "\n",
        "class ToTensor(MTTransform):\n",
        "    \"\"\"Convert a PIL image or numpy array to a PyTorch tensor.\"\"\"\n",
        "\n",
        "    def __init__(self, labeled=True):\n",
        "        self.labeled = labeled\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "\n",
        "        if isinstance(input_data, list):\n",
        "            ret_input = [F.to_tensor(item)\n",
        "                         for item in input_data]\n",
        "        else:\n",
        "            ret_input = F.to_tensor(input_data)\n",
        "\n",
        "        rdict['input'] = ret_input\n",
        "\n",
        "        if self.labeled:\n",
        "            gt_data = sample['gt']\n",
        "            if gt_data is not None:\n",
        "                if isinstance(gt_data, list):\n",
        "                    ret_gt = [F.to_tensor(item)\n",
        "                              for item in gt_data]\n",
        "                else:\n",
        "                    ret_gt = F.to_tensor(gt_data)\n",
        "\n",
        "                rdict['gt'] = ret_gt\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class ToPIL(MTTransform):\n",
        "    def __init__(self, labeled=True):\n",
        "        self.labeled = labeled\n",
        "\n",
        "    def sample_transform(self, sample_data):\n",
        "        # Numpy array\n",
        "        if not isinstance(sample_data, np.ndarray):\n",
        "            input_data_npy = sample_data.numpy()\n",
        "        else:\n",
        "            input_data_npy = sample_data\n",
        "\n",
        "        input_data_npy = np.transpose(input_data_npy, (1, 2, 0))\n",
        "        input_data_npy = np.squeeze(input_data_npy, axis=2)\n",
        "        input_data = Image.fromarray(input_data_npy, mode='F')\n",
        "        return input_data\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "\n",
        "        if isinstance(input_data, list):\n",
        "            ret_input = [self.sample_transform(item)\n",
        "                         for item in input_data]\n",
        "        else:\n",
        "            ret_input = self.sample_transform(input_data)\n",
        "\n",
        "        rdict['input'] = ret_input\n",
        "\n",
        "        if self.labeled:\n",
        "            gt_data = sample['gt']\n",
        "\n",
        "            if isinstance(gt_data, list):\n",
        "                ret_gt = [self.sample_transform(item)\n",
        "                          for item in gt_data]\n",
        "            else:\n",
        "                ret_gt = self.sample_transform(gt_data)\n",
        "\n",
        "            rdict['gt'] = ret_gt\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class res(MTTransform):\n",
        "    \"\"\"Make a center crop of a specified size.\n",
        "\n",
        "    :param segmentation: if it is a segmentation task.\n",
        "                         When this is True (default), the crop\n",
        "                         will also be applied to the ground truth.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, labeled=True):\n",
        "        self.size = size\n",
        "        self.labeled = labeled\n",
        "\n",
        "    @staticmethod\n",
        "    def propagate_params(sample, params):\n",
        "        input_metadata = sample['input_metadata']\n",
        "        input_metadata[\"__centercrop\"] = params\n",
        "        return input_metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(sample):\n",
        "        input_metadata = sample['input_metadata']\n",
        "        return input_metadata[\"__centercrop\"]\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "\n",
        "        w, h = input_data.size\n",
        "        th, tw = self.size\n",
        "        fh = int(round((h - th) / 2.))\n",
        "        fw = int(round((w - tw) / 2.))\n",
        "\n",
        "        params = (fh, fw, w, h)\n",
        "        self.propagate_params(sample, params)\n",
        "\n",
        "        input_data = torchvision.transforms.functional.resize(input_data, self.size)\n",
        "        rdict['input'] = input_data\n",
        "\n",
        "        if self.labeled:\n",
        "            gt_data = sample['gt']\n",
        "            gt_metadata = sample['gt_metadata']\n",
        "            gt_data = torchvision.transforms.functional.resize(gt_data, self.size)\n",
        "            gt_metadata[\"__centercrop\"] = (fh, fw, w, h)\n",
        "            rdict['gt'] = gt_data\n",
        "\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "    def undo_transform(self, sample):\n",
        "        rdict = {}\n",
        "        input_data = sample['input']\n",
        "        fh, fw, w, h = self.get_params(sample)\n",
        "        th, tw = self.size\n",
        "\n",
        "        pad_left = fw\n",
        "        pad_right = w - pad_left - tw\n",
        "        pad_top = fh\n",
        "        pad_bottom = h - pad_top - th\n",
        "\n",
        "        padding = (pad_left, pad_top, pad_right, pad_bottom)\n",
        "        input_data = F.pad(input_data, padding)\n",
        "        rdict['input'] = input_data\n",
        "\n",
        "        sample.update(rdict)\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYDuFIX_Q7Vb"
      },
      "source": [
        "# for custom dataset class\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import glob\n",
        "import nibabel as nib\n",
        "import os\n",
        "from medicaltorch import datasets as mt_datasets\n",
        "from medicaltorch import transforms as mt_transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VFL9saqRMoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5dbb8c-9742-497d-e354-bb74f3244e7e"
      },
      "source": [
        "drive.mount('/gdrive')\n",
        "ROOT_DIR = \"/gdrive/My Drive/MM-WHS 2017 Dataset/\"\n",
        "# ROOT_DIR = 'drive/MyDrive/MM-WHS 2017 Dataset/'\n",
        "mri_input_filename = os.path.join(ROOT_DIR,'ct_train',\n",
        "                                          'ct_train_1001_image.nii.gz')\n",
        "mri_gt_filename = os.path.join(ROOT_DIR,'ct_train',\n",
        "                                          'ct_train_1001_label.nii.gz')\n",
        "\n",
        "pair = mt_datasets.SegmentationPair2D(mri_input_filename, mri_gt_filename)\n",
        "slice_pair = pair.get_pair_slice(159)\n",
        "input_slice = slice_pair[\"input\"]\n",
        "gt_slice = slice_pair[\"gt\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ZTOMfiRRz7"
      },
      "source": [
        "# print(input_slice.shape)\n",
        "# img = input_slice\n",
        "# plt.imshow(img,cmap = 'gray')\n",
        "# plt.show()\n",
        "# img = gt_slice\n",
        "# plt.imshow(img,cmap = 'gray')\n",
        "# plt.show()\n",
        "\n",
        "# img_data,seg_data = pair.get_pair_data()\n",
        "# img_data.shape\n",
        "# seg_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmPgidEuYaIE"
      },
      "source": [
        "\n",
        "img_list = sorted(glob.glob(os.path.join(ROOT_DIR, \"mr_train\")+\"/*image.nii.gz\"))\n",
        "label_list = sorted(glob.glob(os.path.join(ROOT_DIR, \"mr_train\")+\"/*label.nii.gz\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVyOhuQIYbrs"
      },
      "source": [
        "filename_pairs = [(os.path.join(ROOT_DIR,'mr_train',x),os.path.join(ROOT_DIR,'mr_train',y)) for x,y in zip(img_list,label_list)]\n",
        "\n",
        "img_list2 = sorted(glob.glob(os.path.join(ROOT_DIR, \"ct_train\")+\"/*image.nii.gz\"))\n",
        "label_list2 = sorted(glob.glob(os.path.join(ROOT_DIR, \"ct_train\")+\"/*label.nii.gz\"))\n",
        "\n",
        "filename_pairs2 = [(os.path.join(ROOT_DIR,'ct_train',x),os.path.join(ROOT_DIR,'ct_train',y)) for x,y in zip(img_list2,label_list2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bkjaCkzZwIH",
        "outputId": "d5ac36f5-2908-4778-fb2a-2a50f9d503b2"
      },
      "source": [
        "from torchvision import transforms, utils\n",
        "from skimage import io, transform\n",
        "\n",
        "'''\n",
        "Subset data to ony get 1 sample out\n",
        "'''\n",
        "dat=filename_pairs[0]\n",
        "lbl=filename_pairs2[0]\n",
        "\n",
        "filename_pairs.clear()\n",
        "filename_pairs2.clear()\n",
        "\n",
        "filename_pairs.append(dat)\n",
        "filename_pairs2.append(lbl)\n",
        "\n",
        "print(filename_pairs)\n",
        "\n",
        "# print(type(filename_pairs[0]))\n",
        "# print(len(filename_pairs2[0]))\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "\n",
        "        # transforms.Resize((256,256)),\n",
        "        # transforms.ToPILImage(),\n",
        "        # transforms.ToTensor()\n",
        "        res((65,65)),\n",
        "        # res((129,129)),\n",
        "        mt_transforms.ToTensor()\n",
        "        \n",
        "        ]\n",
        ")\n",
        "\n",
        "train_dataset = mt_datasets.MRI2DSegmentationDataset(filename_pairs,transform=train_transform)\n",
        "train_dataset2= mt_datasets.MRI2DSegmentationDataset(filename_pairs2,transform=train_transform)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('/gdrive/My Drive/MM-WHS 2017 Dataset/mr_train/mr_train_1001_image.nii.gz', '/gdrive/My Drive/MM-WHS 2017 Dataset/mr_train/mr_train_1001_label.nii.gz')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSOXVsPaIt0"
      },
      "source": [
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=1,collate_fn=mt_datasets.mt_collate)\n",
        "dataloader2= DataLoader(train_dataset2, batch_size=1, collate_fn=mt_datasets.mt_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "MXuUGpxqtJw2",
        "outputId": "b0d45081-a468-41fa-bc9b-13b6fd2d01ec"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.rcParams.update({'font.size': 5})\n",
        "from itertools import islice\n",
        "# ls = list(zip(dataloader, dataloader2))\n",
        "# print(len(ls))\n",
        "for _ in range(1):\n",
        "    # for el in ls:\n",
        "    for i, batch in islice(zip(dataloader, dataloader2), 80, 81, None):\n",
        "        \n",
        "        # plt.imshow(batch[0][\"input\"][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        # plt.tight_layout()\n",
        "        # plt.show()\n",
        "\n",
        "        '''\n",
        "        MRI IMAGE\n",
        "        '''\n",
        "        # print(batch)\n",
        "        plt.figure(figsize = (1.5,1.5), dpi=128)\n",
        "        plt.imshow(i['input'][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        # print(batch[0]['input'][0].shape)\n",
        "\n",
        "        '''\n",
        "        MRI seg mask\n",
        "        '''\n",
        "\n",
        "        plt.figure(figsize = (1.5,1.5), dpi=128)\n",
        "        plt.imshow(i[\"gt\"][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        '''\n",
        "        CT image\n",
        "        '''\n",
        "\n",
        "        plt.figure(figsize = (1.5,1.5), dpi=128)\n",
        "        plt.imshow(batch[\"input\"][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        '''\n",
        "        CT seg mask\n",
        "        '''\n",
        "        plt.figure(figsize = (1.5,1.5), dpi=128)\n",
        "        plt.imshow(batch[\"gt\"][0].detach().cpu().permute(1,2,0)[:,:,0],cmap = 'gray')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # break\n",
        "'''\n",
        "visualize images\n",
        "'''\n",
        "def vis(image):\n",
        "    plt.imshow(image[0].detach().cpu().permute(1,2,0)[:,:,0],cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:132: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 192x192 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 192x192 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 192x192 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 192x192 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATdyNeDuYdFm"
      },
      "source": [
        "# https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/21\n",
        "# https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/random_ops.py#L163\n",
        "\n",
        "def truncated_normal(t, mean=0.0, std=0.01):\n",
        "    torch.nn.init.normal_(t, mean=mean, std=std)\n",
        "    while True:\n",
        "      cond = torch.logical_or(t < mean - 2*std, t > mean + 2*std)\n",
        "      if not torch.sum(cond):\n",
        "        break\n",
        "      t = torch.where(cond.cuda(), torch.nn.init.normal_(torch.ones(t.shape), mean=mean, std=std).cuda(), t.cuda()).cuda()\n",
        "      \n",
        "    # return t\n",
        "\n",
        "def compute_conv_output(H, K, S):\n",
        "  return int(((H-K+2*0)/S)+1)\n",
        "\n",
        "def compute_padding(out, H, K, S):\n",
        "  return int((S*(out-1)-H+K)/2)\n",
        "\n",
        "def compute_padding_deconv(out, H, K, S):\n",
        "  return int((-out+(H-1)*S+K)/2)\n",
        "\n",
        "def compute_maxpool_padding(S, H, f):\n",
        "  '''\n",
        "  H2=(H1-f)/s+1\n",
        "  '''\n",
        "  return int((S*(H-1)-H+f)/2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l53qurIPBILl"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nswSWccwQI-5"
      },
      "source": [
        "\n",
        "class General_Conv2D(nn.Module):\n",
        "    # @torch.no_grad()\n",
        "    def init_weights(self, n):\n",
        "      if type(n) == nn.Conv2d:\n",
        "          truncated_normal(t=n.weight, std=0.01)\n",
        "          \n",
        "\n",
        "    def __init__(self, in_features=0, out_features=64, k=7, s=1, stddev=0.01, do_relu=True, keep_rate=None, relu_factor=0, norm_type=None, train=True, padding=False, input_dim=0):\n",
        "        super(General_Conv2D, self).__init__()\n",
        "        self.std=stddev\n",
        "        self.keep_rate=keep_rate\n",
        "        self.relu_factor=relu_factor\n",
        "        self.norm_type=norm_type\n",
        "        self.do_relu=do_relu\n",
        "        self.k=k\n",
        "        self.s=s\n",
        "        self.padding=padding\n",
        "        self.stddev = stddev\n",
        "\n",
        "        if not keep_rate is None:\n",
        "          self.dropout = nn.Dropout(p=keep_rate, inplace=True)\n",
        "        \n",
        "        if norm_type=='Batch':\n",
        "          # if train:\n",
        "            # self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True).train()\n",
        "          # else:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True)\n",
        "        elif norm_type=='Ins':\n",
        "          # if train:\n",
        "            # self.instancenorm=nn.InstanceNorm2d(num_features=out_features).train()\n",
        "          # else:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features)\n",
        "        \n",
        "        self.relu=nn.ReLU(inplace=True)\n",
        "        self.lrealu=nn.LeakyReLU(inplace=True)\n",
        "        \n",
        "        self.w= nn.Parameter(torch.randn(out_features, in_features, k,k))\n",
        "        truncated_normal(t=self.w, std=self.stddev)\n",
        "        self.b=nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        initialize weights\n",
        "        '''\n",
        "        \n",
        "        if self.padding==False:\n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=0)\n",
        "\n",
        "        else:\n",
        "          \n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=compute_padding(x.shape[-1], x.shape[-1], self.k, self.s))\n",
        "        \n",
        "        '''\n",
        "        dropout\n",
        "        '''\n",
        "        if not self.keep_rate is None:\n",
        "          x=self.dropout(x)\n",
        "\n",
        "        '''\n",
        "        norm\n",
        "        '''\n",
        "        if (self.norm_type=='Batch'):\n",
        "          x=self.batchnorm(x)\n",
        "        elif (self.norm_type=='Ins'):\n",
        "          x=self.instancenorm(x)\n",
        "\n",
        "        '''\n",
        "        act fnc\n",
        "        '''\n",
        "        if (self.do_relu==True):\n",
        "          if (self.relu_factor==0):\n",
        "            x=self.relu(x)\n",
        "          else:\n",
        "            x=self.lrealu(x)\n",
        "        \n",
        "        \n",
        "\n",
        "        return x\n",
        "\n",
        "        # return x + self.block(x)\n",
        "\n",
        "\n",
        "class Resnet_Block(nn.Module):\n",
        "    def __init__(self, in_features, out_features , padding=\"REFLECT\", norm_type=None, keep_rate=0.75):\n",
        "        super(Resnet_Block, self).__init__()\n",
        "\n",
        "        if padding=='REFLECT':\n",
        "          pad_layer=nn.ReflectionPad2d(1)\n",
        "        elif padding=='CONSTANT':\n",
        "          pad_layer=nn.ZeroPad2d(1)\n",
        "        else: # SYMMETRIC\n",
        "          pad_layer=nn.ReplicationPad2d(1)\n",
        "\n",
        "        self.block = nn.ModuleList([\n",
        "            pad_layer,\n",
        "            General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, norm_type=norm_type, keep_rate=keep_rate, padding=False),\n",
        "            pad_layer,\n",
        "            General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, do_relu=False, norm_type=norm_type, keep_rate=keep_rate, padding=False)\n",
        "        ])\n",
        "\n",
        "      \n",
        "        self.relu=nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):      \n",
        "\n",
        "        for i in range(len(self.block)):\n",
        "          if i==0:\n",
        "            out = self.block[i](x)\n",
        "          else:\n",
        "            out = self.block[i](out)\n",
        "\n",
        "  \n",
        "        return self.relu(out+x)\n",
        "\n",
        "class Resnet_Block_ds(nn.Module):\n",
        "    def __init__(self, in_features=0, out_features=0, padding=\"REFLECT\",dim=0, norm_type=None, keep_rate=None):\n",
        "        super(Resnet_Block_ds, self).__init__()\n",
        "\n",
        "        self.in_features=in_features\n",
        "        self.out_features=out_features\n",
        "\n",
        "        if padding=='REFLECT':\n",
        "          pad_layer=nn.ReflectionPad2d(1)\n",
        "        elif padding=='CONSTANT':\n",
        "          pad_layer=nn.ZeroPad2d(1)\n",
        "        else: # SYMMETRIC\n",
        "          pad_layer=nn.ReplicationPad2d(1)\n",
        "\n",
        "\n",
        "        self.block = nn.ModuleList([\n",
        "            \n",
        "            pad_layer,\n",
        "            General_Conv2D(in_features, out_features, k=3, s=1, stddev=0.01, norm_type=norm_type, keep_rate=keep_rate, padding=False),\n",
        "            pad_layer,\n",
        "            General_Conv2D(out_features, out_features, k=3, s=1, stddev=0.01, do_relu=False, norm_type=norm_type, keep_rate=keep_rate, padding=False)\n",
        "        ])\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "     \n",
        "        for i in range(len(self.block)):\n",
        "          if i==0:\n",
        "            out = self.block[i](x)\n",
        "          else:\n",
        "            out = self.block[i](out)\n",
        "        '''\n",
        "        pad channel dim\n",
        "        '''\n",
        "        pd=(self.out_features-self.in_features) // 2\n",
        "\n",
        "        padding = torch.zeros(x.shape[0], pd, x.shape[2], x.shape[3]).cuda()\n",
        "        padded_inp = torch.cat((x, padding), 1)\n",
        "        padded_inp = torch.cat((padding, padded_inp), 1)\n",
        "\n",
        "\n",
        "        return self.relu(out+padded_inp)\n",
        "  \n",
        "class drn_Block(nn.Module):\n",
        "    def __init__(self, in_features, keep_rate=None):\n",
        "        super(drn_Block, self).__init__()\n",
        "# TODO remove hard coded\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(2),\n",
        "            nn.Conv2d(in_features, in_features, 3, dilation=2),\n",
        "            nn.Dropout(1.0, inplace=True),\n",
        "            nn.BatchNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(2),\n",
        "            nn.Conv2d(in_features, in_features, 3, dilation=2),\n",
        "            nn.Dropout(1.0, inplace=True),\n",
        "            nn.BatchNorm2d(in_features),\n",
        "        )\n",
        "\n",
        "        self.relu=nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.block(x)\n",
        "        \n",
        "\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "\n",
        "class General_Conv2D_GA(nn.Module):\n",
        "    # @torch.no_grad()\n",
        "    def init_weights(self, n):\n",
        "      if type(n) == nn.Conv2d:\n",
        "          truncated_normal(n.weight, 0.02)\n",
        "          \n",
        "\n",
        "    def __init__(self, in_features=0, out_features=64, k=7, s=1, stddev=0.02, do_relu=True, keep_rate=None, relu_factor=0, norm_type=None, train=True, padding=False, input_dim=0):\n",
        "        super(General_Conv2D_GA, self).__init__()\n",
        "        self.std=stddev\n",
        "        self.keep_rate=keep_rate\n",
        "        self.relu_factor=relu_factor\n",
        "        self.norm_type=norm_type\n",
        "        self.do_relu=do_relu\n",
        "        self.k=k\n",
        "        self.s=s\n",
        "        self.padding=padding\n",
        "        self.stddev=stddev\n",
        "\n",
        "        if not keep_rate is None:\n",
        "          self.dropout = nn.Dropout(p=keep_rate, inplace=True)\n",
        "        \n",
        "        if norm_type=='Batch':\n",
        "          if train:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True).train()\n",
        "          else:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True)\n",
        "        elif norm_type=='Ins':\n",
        "          if train:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features).train()\n",
        "          else:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features)\n",
        "        \n",
        "        self.relu=nn.ReLU(inplace=True)\n",
        "        self.lrealu=nn.LeakyReLU(inplace=True)\n",
        "        \n",
        "        self.w= nn.Parameter(torch.randn(out_features, in_features, k,k))\n",
        "        truncated_normal(t=self.w, std=self.stddev)\n",
        "        self.b=nn.Parameter(torch.randn(out_features))\n",
        "        torch.nn.init.constant_(self.b, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        initialize weights\n",
        "        '''\n",
        "     \n",
        "        if self.padding==False:\n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=0)\n",
        "\n",
        "        else:\n",
        "          \n",
        "          x = torch.nn.functional.conv2d(x, self.w, self.b, padding=compute_padding(x.shape[-1], x.shape[-1], self.k, self.s))\n",
        "        \n",
        "        '''\n",
        "        dropout\n",
        "        # '''\n",
        "        if not self.keep_rate is None:\n",
        "          x=self.dropout(x)\n",
        "\n",
        "        '''\n",
        "        norm\n",
        "        '''\n",
        "        if (self.norm_type=='Batch'):\n",
        "          x=self.batchnorm(x)\n",
        "        elif self.norm_type=='Ins':# Instance norm\n",
        "          x=self.instancenorm(x)\n",
        "\n",
        "        '''\n",
        "        act fnc\n",
        "        '''\n",
        "        if (self.do_relu==True):\n",
        "          if (self.relu_factor==0):\n",
        "            x=self.relu(x)\n",
        "          else:\n",
        "            x=self.lrealu(x)\n",
        "        \n",
        "       \n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resnet_Block_ins(nn.Module):\n",
        "    def __init__(self, in_features=0, out_features=0, padding=\"REFLECT\",dim=0, norm_type=None):\n",
        "        super(Resnet_Block_ins, self).__init__()\n",
        "\n",
        "        self.in_features=in_features\n",
        "        self.out_features=out_features\n",
        "\n",
        "        if padding=='REFLECT':\n",
        "          pad_layer=nn.ReflectionPad2d(1)\n",
        "        elif padding=='CONSTANT':\n",
        "          pad_layer=nn.ZeroPad2d(1)\n",
        "        else: # SYMMETRIC\n",
        "          pad_layer=nn.ReplicationPad2d(1)\n",
        "\n",
        "        self.block = nn.ModuleList([\n",
        "            \n",
        "            pad_layer,\n",
        "            General_Conv2D_GA(in_features, out_features, k=3, s=1, stddev=0.02, norm_type='Ins', padding=False, do_relu=True),\n",
        "            pad_layer,\n",
        "            General_Conv2D_GA(out_features, out_features, k=3, s=1, stddev=0.02, norm_type='Ins', padding=False, do_relu=False)\n",
        "\n",
        "            ]\n",
        "        )\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      y=x\n",
        "\n",
        "      for i in range(len(self.block)):\n",
        "        if i==0:\n",
        "          out=self.block[i](x)\n",
        "        else:\n",
        "          out=self.block[i](out)\n",
        "      # out=out.add(y)\n",
        "      return self.relu(out+y)\n",
        "      # return self.relu(torch.cat((out,y)))\n",
        "\n",
        "      # for i in range(len(self.block)):\n",
        "      #     x=self.block[i](x)\n",
        "      \n",
        "      # return self.relu(torch.cat((x,y)))\n",
        "class General_Deconv2D(nn.Module):\n",
        "    # @torch.no_grad()\n",
        "    def init_weights(self, n):\n",
        "      if type(n) == nn.Conv2d:\n",
        "          truncated_normal(n.weight, 0.02)\n",
        "          \n",
        "\n",
        "    def __init__(self, in_features=0, out_features=64, k=7, s=1, stddev=0.02, do_relu=True, keep_rate=None, relu_factor=0, norm_type=None, train=True, padding=False, input_dim=0, out_shape=0):\n",
        "        super(General_Deconv2D, self).__init__()\n",
        "        self.std=stddev\n",
        "        self.keep_rate=keep_rate\n",
        "        self.relu_factor=relu_factor\n",
        "        self.norm_type=norm_type\n",
        "        self.do_relu=do_relu\n",
        "        self.k=k\n",
        "        self.s=s\n",
        "        self.padding=padding\n",
        "        self.out_shape=out_shape\n",
        "        self.stddev=stddev\n",
        "        \n",
        "        if norm_type=='Batch':\n",
        "          if train:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True).train()\n",
        "          else:\n",
        "            self.batchnorm=nn.BatchNorm2d(num_features=out_features, momentum=0.1, affine=True)\n",
        "        elif norm_type=='Ins':\n",
        "          if train:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features).train()\n",
        "          else:\n",
        "            self.instancenorm=nn.InstanceNorm2d(num_features=out_features)\n",
        "        \n",
        "        self.relu=nn.ReLU(inplace=True)\n",
        "        self.lrealu=nn.LeakyReLU(inplace=True)\n",
        "        \n",
        "        self.w= nn.Parameter(torch.randn(in_features, out_features, k,k))\n",
        "        truncated_normal(self.w, std=self.stddev)\n",
        "        self.b=nn.Parameter(torch.randn(out_features))\n",
        "        torch.nn.init.constant_(self.b, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.padding==False:\n",
        "          x = torch.nn.functional.conv_transpose2d(x, self.w, bias=self.b, stride=self.s, padding=0)\n",
        "          # x = torch.nn.functional.conv2d(x, self.w, self.b, padding=0)\n",
        "\n",
        "        else:\n",
        "          x = torch.nn.functional.conv_transpose2d(x, self.w, bias=self.b, stride=self.s, padding=compute_padding_deconv(self.out_shape, x.shape[-1], self.k, self.s))\n",
        "          # x = torch.nn.functional.conv2d(x, self.w, self.b, padding=compute_padding(x.shape[-1], x.shape[-1], self.k, self.s))\n",
        "        \n",
        "\n",
        "        '''\n",
        "        norm\n",
        "        '''\n",
        "        if (self.norm_type=='Batch'):\n",
        "          x=self.batchnorm(x)\n",
        "        else:# Instance norm\n",
        "          x=self.instancenorm(x)\n",
        "\n",
        "        '''\n",
        "        act fnc\n",
        "        '''\n",
        "        if (self.do_relu==True):\n",
        "          if (self.relu_factor==0):\n",
        "            x=self.relu(x)\n",
        "          else:\n",
        "            x=self.lrealu(x)\n",
        "\n",
        "\n",
        "        'explicit resize'\n",
        "        # x=x[:,:,:self.out_shape,:self.out_shape]\n",
        "        \n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVPIOpijFi2C"
      },
      "source": [
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, skip=True):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        self.skip=skip\n",
        "        # self.input_img=input_img\n",
        "        self.max_features = 32*4 # used to be 32*4 but there were memory issues.\n",
        "        \n",
        "        self.block = nn.ModuleList([\n",
        "            nn.ZeroPad2d(3),\n",
        "            General_Conv2D_GA(in_features=1, out_features=32, k=7, s=1, stddev=0.02, do_relu=True, norm_type='Ins', train=True, padding=False),\n",
        "            General_Conv2D_GA(in_features=32, out_features=32*2, k=3, s=2, stddev=0.02, do_relu=True, norm_type='Ins', train=True, padding=True),\n",
        "            General_Conv2D_GA(in_features=32*2, out_features=self.max_features, k=3, s=2, stddev=0.02, do_relu=True, norm_type='Ins', train=True, padding=True),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            Resnet_Block_ins(in_features=self.max_features, out_features=self.max_features, padding='CONSTANT'),\n",
        "            General_Deconv2D(in_features=self.max_features, out_features=32*2, k=3, s=2, stddev=0.02, norm_type='Ins', out_shape=32, padding=True),\n",
        "            General_Deconv2D(in_features=32*2, out_features=32, k=3, s=2, stddev=0.02, norm_type='Ins', out_shape=64, padding=True),\n",
        "            General_Conv2D_GA(in_features=32, out_features=1, k=7, s=1, stddev=0.02, do_relu=False, train=True, padding=True),\n",
        "            ]\n",
        "            )\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.pad = torch.nn.ReflectionPad2d((1,0,1,0))\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      y=x\n",
        "\n",
        "      for i in range(len(self.block)):\n",
        "        if i==0:\n",
        "          out=self.block[i](x)\n",
        "        else:\n",
        "          out=self.block[i](out)\n",
        "\n",
        "      # y=self.pad(y)\n",
        "      if self.skip==True:\n",
        "        return self.tanh(out+y)\n",
        "      else:\n",
        "        return self.tanh(out)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2wCespUOIZj"
      },
      "source": [
        "'''\n",
        "input shape: B,C,H,W\n",
        "'''\n",
        "class EncoderNet(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(EncoderNet, self).__init__()\n",
        "        \n",
        "        self.input_shape=input_shape\n",
        "\n",
        "  \n",
        "\n",
        "        self.out_c1 = General_Conv2D(in_features=1, out_features=16, padding=True, input_dim=self.input_shape[-1], norm_type='Batch', keep_rate=1.0)\n",
        "        self.out_res1 = Resnet_Block(in_features=16,out_features=16,norm_type='Batch', keep_rate=1.0)\n",
        "        self.out1 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=input_shape[-1], f=1))\n",
        "        \n",
        "        self.out_res2 = Resnet_Block_ds(in_features=16, out_features=16*2, norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "\n",
        "        self.out_res3 = Resnet_Block_ds(in_features=16*2, out_features=16*4, norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "        self.out_res4 = Resnet_Block(in_features=16*4,out_features=16*4,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "\n",
        "        self.out_res5 = Resnet_Block_ds(in_features=16*4,out_features=16*8,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "        self.out_res6 = Resnet_Block(in_features=16*8,out_features=16*8,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "\n",
        "        self.out_res7 = Resnet_Block_ds(in_features=16*8,out_features=16*16,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "        self.out_res8 = Resnet_Block(in_features=16*16,out_features=16*16,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "\n",
        "        self.out_res9 = Resnet_Block(in_features=16*16,out_features=16*16,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "        self.out_res10= Resnet_Block(in_features=16*16,out_features=16*16,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "\n",
        "        self.out_res11= Resnet_Block_ds(in_features=16*16,out_features=16*32,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "        self.out_res12= Resnet_Block(in_features=16*32,out_features=16*32,norm_type='Batch', padding='CONSTANT', keep_rate=1.0)\n",
        "\n",
        "        self.out_drn1 = drn_Block(in_features=16*32, keep_rate=1.0)\n",
        "        self.out_drn2 = drn_Block(in_features=16*32, keep_rate=1.0)\n",
        "        \n",
        "        self.out_c2 = General_Conv2D(in_features=16*32, out_features=16*32, k=3, padding=True, input_dim=0, norm_type='Batch', keep_rate=1.0)\n",
        "        self.out_c3 = General_Conv2D(in_features=16*32, out_features=16*32, k=3, padding=True, input_dim=0, norm_type='Batch', keep_rate=1.0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "  \n",
        "        '''\n",
        "        separate kernel size implementation from tf not available in pytorch\n",
        "        TODO\n",
        "        '''\n",
        "        x=self.out_c1(x)\n",
        "        x=self.out_res1(x)\n",
        "        x=self.out1(x)\n",
        "        x=self.out_res2(x)\n",
        "        x=torch.nn.functional.max_pool2d(x,kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=x.shape[-1], f=1))\n",
        "        # self.out2 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=x.shape[-1], f=1))\n",
        "        # x=self.out2(x)\n",
        "        x=self.out_res3(x)\n",
        "        x=self.out_res4(x)\n",
        "        x=torch.nn.functional.max_pool2d(x,kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=x.shape[-1], f=1))\n",
        "        # self.out3 = nn.MaxPool2d(kernel_size=1, stride=1, padding=compute_maxpool_padding(S=1, H=x.shape[-1], f=1))\n",
        "        # x=self.out3(x)\n",
        "        x=self.out_res5(x)\n",
        "        x=self.out_res6(x)\n",
        "        x=self.out_res7(x)\n",
        "        x=self.out_res8(x)\n",
        "        x=self.out_res9(x)\n",
        "        x=self.out_res10(x)\n",
        "       \n",
        "        x=self.out_res11(x)\n",
        "        x=self.out_res12(x)\n",
        "        y=x\n",
        "        x=self.out_drn1(x)\n",
        "        x=self.out_drn2(x)\n",
        "        '''\n",
        "        '''\n",
        "        x=self.out_c2(x)\n",
        "        x=self.out_c3(x)\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKN5BOs7OTac"
      },
      "source": [
        "class DecoderNet(nn.Module):\n",
        "    def __init__(self, input_shape, skip=False):\n",
        "        super(DecoderNet, self).__init__()\n",
        "        \n",
        "        self.skip=skip\n",
        "        self.input_shape=input_shape\n",
        "        \n",
        "        self.block = nn.ModuleList([\n",
        "          General_Conv2D(in_features=self.input_shape[1], k=3, s=1, out_features=32*4, padding=True, input_dim=self.input_shape[-1], norm_type='Ins'),\n",
        "          Resnet_Block(in_features=32*4,out_features=32*4, padding='CONSTANT', norm_type='Ins', keep_rate=0.75),\n",
        "          Resnet_Block(in_features=32*4,out_features=32*4,padding='CONSTANT',norm_type='Ins', keep_rate=0.75),\n",
        "          Resnet_Block(in_features=32*4,out_features=32*4,padding='CONSTANT',norm_type='Ins', keep_rate=0.75),\n",
        "          Resnet_Block(in_features=32*4,out_features=32*4,padding='CONSTANT',norm_type='Ins', keep_rate=0.75),\n",
        "          General_Deconv2D(in_features=32*4, out_features=32*2, k=3, s=2, stddev=0.02, norm_type='Ins', out_shape=16, padding=True),\n",
        "          General_Deconv2D(in_features=32*2, out_features=32*2, k=3, s=2, stddev=0.02, norm_type='Ins', out_shape=32, padding=True),\n",
        "          General_Deconv2D(in_features=32*2, out_features=32, k=3, s=2, stddev=0.02, norm_type='Ins', out_shape=64, padding=True),\n",
        "          General_Conv2D(in_features=32, out_features=1, k=7, s=1, norm_type=None, do_relu=False, stddev=0.02, padding=True, input_dim=self.input_shape[-1])\n",
        "        ])\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, input_img):\n",
        "      for i in range(len(self.block)):\n",
        "        if i==0:\n",
        "          out=self.block[i](x)\n",
        "        else:\n",
        "          out=self.block[i](out)\n",
        "\n",
        "      if self.skip==True:\n",
        "        return self.tanh(input_img+out)\n",
        "      else:\n",
        "        return self.tanh(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6SMox-HHcKs"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape, skip=False):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.skip=skip\n",
        "        self.input_shape=input_shape\n",
        "\n",
        "        self.block = nn.ModuleList([\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=input_shape[0], out_features=64, k=4, s=2, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type=None),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64, out_features=64*2, k=4, s=2, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type='Ins'),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64*2, out_features=64*4, k=4, s=2, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type='Ins'),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64*4, out_features=64*8, k=4, s=1, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type='Ins'),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64*8, out_features=1, k=4, s=1, stddev=0.02, padding=False, input_dim=self.input_shape[-1], norm_type=None, do_relu=False),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      for i in range(len(self.block)):\n",
        "          x=self.block[i](x)\n",
        "      return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmsgqo9hSRqQ"
      },
      "source": [
        "class Discriminator_aux(nn.Module):\n",
        "    def __init__(self, input_shape, skip=False):\n",
        "        super(Discriminator_aux, self).__init__()\n",
        "        \n",
        "        self.skip=skip\n",
        "        self.input_shape=input_shape\n",
        "\n",
        "        self.block = nn.ModuleList([\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=1, out_features=64, k=4, s=2, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type=None),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64, out_features=64*2, k=4, s=2, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type='Ins'),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64*2, out_features=64*4, k=4, s=2, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type='Ins'),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64*4, out_features=64*8, k=4, s=1, stddev=0.02, padding=False, relu_factor=0.02, input_dim=self.input_shape[-1], norm_type='Ins'),\n",
        "          nn.ZeroPad2d(2),\n",
        "          General_Conv2D(in_features=64*8, out_features=1, k=4, s=1, stddev=0.02, padding=False, input_dim=self.input_shape[-1], norm_type=None, do_relu=False),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "      for i in range(len(self.block)):\n",
        "          x=self.block[i](x)\n",
        "      # return out, out\n",
        "      # x1=x[...,0]\n",
        "      # x2=x[...,1]\n",
        "      # return torch.unsqueeze(x1, dim=3), torch.unsqueeze(x2, dim=3)\n",
        "      return torch.unsqueeze(x[...,0], dim=3), torch.unsqueeze(x[...,1], dim=3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeqHN_I3EK9s"
      },
      "source": [
        "class Segmenter(nn.Module):\n",
        "    def __init__(self, keep_rate):\n",
        "      super(Segmenter, self).__init__()\n",
        "\n",
        "      self.l1 = General_Conv2D(in_features=64*8, out_features=5, k=1, s=1, stddev=0.01, padding=True, relu_factor=0, do_relu=False, keep_rate=keep_rate, norm_type=None)\n",
        "      # self.up = torch.nn.Upsample((65,65), mode='bilinear')\n",
        "    def forward(self, x):\n",
        "      x=self.l1(x)\n",
        "      # x=self.up(x)\n",
        "      x=torch.nn.functional.interpolate(x, size=(65,65), mode='bilinear', align_corners=None)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UGPVqQjxiUw"
      },
      "source": [
        "'''\n",
        "Params for cycle loss calculation\n",
        "'''\n",
        "LAMBDA_A=10\n",
        "LAMBDA_B=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws334B5F_MxD"
      },
      "source": [
        "'''\n",
        "Losses\n",
        "'''\n",
        "\n",
        "'''\n",
        "Copute lsgan loss for generator\n",
        "'''\n",
        "def lsgan_loss_generator(prob_fake_is_real):\n",
        "    return (torch.mean((prob_fake_is_real - 1)**2))\n",
        "\n",
        "'''\n",
        "Compute lsgan loss (which needs to be minimized) (lsgan to help with vanishing/exploding gradients)?\n",
        "'''\n",
        "def lsgan_loss_discriminator(prob_real_is_real, prob_fake_is_real):\n",
        "    return (torch.mean((prob_real_is_real - 1)**2) + torch.mean((prob_fake_is_real - 0)**2))*0.5\n",
        "\n",
        "\n",
        "'''\n",
        "Compute cycle loss\n",
        "'''\n",
        "def cycle_consistency_loss(real_images, generated_images):\n",
        "    return (torch.mean(torch.abs(real_images - generated_images)))\n",
        "\n",
        "\n",
        "'''\n",
        "Weighted softmax loss\n",
        "'''\n",
        "def softmax_weighted_loss(logits, gt):\n",
        "  softmax=torch.nn.Softmax()\n",
        "  # softmaxpred = softmax(logits)\n",
        "  # softmaxpred=torch.softmax(logits)\n",
        "  softmaxpred=torch.nn.functional.softmax(logits, dim=1)\n",
        "  for i in range(5):\n",
        "    gti = gt[:,i,:,:]\n",
        "    predi=softmaxpred[:,i,:,:]\n",
        "    weighted = 1-(torch.sum(gti)/torch.sum(gt))\n",
        "    if i==0:\n",
        "      raw_loss = -1.0 * weighted * gti * torch.log(torch.clamp(predi, 0.005, 1))\n",
        "    else:\n",
        "      raw_loss += -1.0 * weighted * gti * torch.log(torch.clamp(predi, 0.005, 1))\n",
        "  loss=torch.mean(raw_loss)\n",
        "  return loss\n",
        "'''\n",
        "Dice loss\n",
        "'''\n",
        "def _dice_loss(logits, gt):\n",
        "  dice=0\n",
        "  eps=1e-7\n",
        "  # softmax=torch.nn.Softmax()\n",
        "  # softmaxpred = softmax(logits)\n",
        "  # softmaxpred=torch.softmax(logits, dim=1)\n",
        "  softmaxpred=torch.nn.functional.softmax(logits, dim=1)\n",
        "  for i in range(5):\n",
        "    inse = torch.sum(softmaxpred[:,i,:,:]*gt[:,i,:,:])\n",
        "    l = torch.sum(softmaxpred[:,i,:,:]*softmaxpred[:,i,:,:])\n",
        "    r = torch.sum(gt[:,i,:,:])\n",
        "    dice += 2.0 * inse/(l+r+eps)\n",
        "  return 1-1.0*dice/5\n",
        "'''\n",
        "Task loss:\n",
        "cross entropy loss + dice loss\n",
        "'''\n",
        "def task_loss(prediction, gt):\n",
        "  ce_loss = softmax_weighted_loss(prediction, gt)\n",
        "  dice_loss = _dice_loss(prediction, gt)\n",
        "  return ce_loss, dice_loss\n",
        "\n",
        "\n",
        "'''\n",
        "l2 loss\n",
        "'''\n",
        "def l2_loss():\n",
        "  tensors=[]\n",
        "  for param in segmenter.parameters():\n",
        "    tensors.append(0.0001*(torch.sum(param.data**2)/2))\n",
        "  for param in segmenter_II.parameters():\n",
        "    tensors.append(0.0001*(torch.sum(param.data**2)/2))\n",
        "  for param in Encoder.parameters():\n",
        "    tensors.append(0.0001*(torch.sum(param.data**2)/2))\n",
        "  re=0.0\n",
        "  for val in tensors:\n",
        "    re+=val\n",
        "  return 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "FrKIcdHO3OHc",
        "outputId": "08d02f8c-3c5e-4d91-846a-6a34b3507335"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import datetime\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "'''\n",
        "Encoder\n",
        "'''\n",
        "Encoder = EncoderNet(input_shape=(2,1,64,64))\n",
        "'''\n",
        "Decoder\n",
        "'''\n",
        "Decoder = DecoderNet(input_shape=(1,512,64,64), skip=True)\n",
        "\n",
        "'''\n",
        "Discriminator\n",
        "'''\n",
        "discriminator = Discriminator((1,64,64))\n",
        "discriminator_aux = Discriminator_aux((1,64,64))\n",
        "\n",
        "'''\n",
        "Generator\n",
        "'''\n",
        "G_A=GeneratorResNet(skip=True)\n",
        "\n",
        "'''\n",
        "Segmenter\n",
        "'''\n",
        "segmenter=Segmenter(keep_rate=1.0)\n",
        "segmenter_II=Segmenter(keep_rate=1.0)\n",
        "\n",
        "'''\n",
        "Discriminator (seg)\n",
        "'''\n",
        "discriminator_p = Discriminator((5,64,64))\n",
        "discriminator_p_II = Discriminator((5,64,64))\n",
        "\n",
        "if cuda:\n",
        "    Encoder = Encoder.cuda()\n",
        "    Decoder = Decoder.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    discriminator_aux=discriminator_aux.cuda()\n",
        "    G_A=G_A.cuda()\n",
        "    segmenter=segmenter.cuda()\n",
        "    segmenter_II=segmenter_II.cuda()\n",
        "    discriminator_p=discriminator_p.cuda()\n",
        "    discriminator_p_II=discriminator_p_II.cuda()\n",
        "\n",
        "\n",
        "opt={\"lr\":0.0002,\"b1\":0.5, \"b2\":0.999, \"epoch\":0, \"n_epochs\":20000, \"decay_epoch\":5, \"lambda_cyc\":10.0, \"lambda_id\":5.0}\n",
        "\n",
        "\n",
        "optimizer_G = torch.optim.Adam(itertools.chain(G_A.parameters(), Decoder.parameters()), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "# optimizer_G_A = torch.optim.Adam(G_A.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "# optimizer_G_B = torch.optim.Adam(Decoder.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "optimizer_D_A = torch.optim.Adam(discriminator_aux.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "optimizer_D_B = torch.optim.Adam(discriminator.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "optimizer_seg = torch.optim.Adam(itertools.chain(segmenter.parameters(), segmenter_II.parameters(), Encoder.parameters()), lr=opt[\"lr\"])\n",
        "optimizer_D_P = torch.optim.Adam(discriminator_p.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "optimizer_D_P_II=torch.optim.Adam(discriminator_p_II.parameters(), lr=opt[\"lr\"], betas=(opt[\"b1\"], opt[\"b2\"]))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "G_A.train()\n",
        "discriminator.train()\n",
        "Encoder.train()\n",
        "Decoder.train()\n",
        "discriminator_aux.train()\n",
        "segmenter.train()\n",
        "segmenter_II.train()\n",
        "discriminator_p.train()\n",
        "discriminator_p_II.train()\n",
        "\n",
        "'''\n",
        "Training loop starts\n",
        "'''\n",
        "for epoch in range(opt[\"n_epochs\"]):\n",
        "    for i, batch in islice(zip(dataloader, dataloader2),80, 81, None):\n",
        "      #  \n",
        "        # if (i==1): # trying to overfit on single image\n",
        "          # i=0\n",
        "          # break\n",
        "        G_A.train()\n",
        "        discriminator.train()\n",
        "        Encoder.train()\n",
        "        Decoder.train()\n",
        "        discriminator_aux.train()\n",
        "        segmenter.train()\n",
        "        segmenter_II.train()\n",
        "        discriminator_p.train()\n",
        "        discriminator_p_II.train()\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "          \n",
        "\n",
        "          real_A = Variable(i[\"input\"].type(Tensor)) # mri\n",
        "          real_B = Variable(batch[\"input\"].type(Tensor)) # ct\n",
        "\n",
        "          gt_A = Variable(i[\"gt\"].type(Tensor)) # mri\n",
        "          gT_B = Variable(batch[\"gt\"].type(Tensor)) # ct\n",
        "\n",
        "          '''\n",
        "          Normalize?\n",
        "          '''\n",
        "          # gt_A = (2*((gt_A - torch.min(gt_A))/(torch.max(gt_A)-torch.min(gt_A))))-1\n",
        "\n",
        "\n",
        "          '''\n",
        "          ??Transform gt labels to have 5 dimensions. Onehot. \n",
        "          '''\n",
        "          print(real_A.shape)\n",
        "          print(gt_A.shape)\n",
        "          gt_A=gt_A.detach().cpu()\n",
        "          gt_A=gt_A.numpy()\n",
        "          np_tensor=gt_A\n",
        "          physical_devices = tf.config.list_physical_devices('GPU')\n",
        "          tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "          tf_tensor=tf.convert_to_tensor(np_tensor)\n",
        "          inter=tf.one_hot(tf.cast(tf.squeeze(tf_tensor), tf.uint8), 5)\n",
        "          gt_A=torch.from_numpy(inter.numpy())\n",
        "          gt_A=gt_A.permute(2,0,1)\n",
        "          gt_A=gt_A.unsqueeze(0)\n",
        "          gt_A=gt_A.to('cuda')\n",
        "          gt_A.requires_grad=True\n",
        "\n",
        "\n",
        "\n",
        "          # TODO uint8 in tf code\n",
        "          # gt_A=gt_A.detach()\n",
        "          # torch.set_printoptions(threshold=10_000)\n",
        "          # y=torch.squeeze(gt_A).type(torch.uint8)\n",
        "          # print(y)\n",
        "          # print(torch.unique(y))\n",
        "          # y=F.one_hot(torch.squeeze(y).type(torch.uint8), num_classes=5)\n",
        "          # print(y)\n",
        "          # vis(y)\n",
        "          # print(torch.unique(y))\n",
        "          # gt_A=F.one_hot(torch.squeeze(gt_A).type(torch.LongTensor), num_classes=5)\n",
        "          \n",
        "          \n",
        "          \n",
        "          '''\n",
        "          Normalize data b/w -1,1 range\n",
        "          '''\n",
        "          real_A = (2*((real_A - torch.min(real_A))/(torch.max(real_A)-torch.min(real_A))))-1\n",
        "          real_B = (2*((real_B - torch.min(real_B))/(torch.max(real_B)-torch.min(real_B))))-1\n",
        "\n",
        "          ################ Computing GAN Losses\n",
        "          '''\n",
        "          Optimizing G_A network\n",
        "          (real_A -> fake_B)\n",
        "          '''\n",
        "\n",
        "          # optimizer_G_A.zero_grad()\n",
        "          # optimizer_G_B.zero_grad()\n",
        "          optimizer_G.zero_grad()\n",
        "          optimizer_seg.zero_grad() # Encoder and seg1 + seg2\n",
        "\n",
        "\n",
        "          fake_imgs_b = G_A(real_A)\n",
        "\n",
        "          '''\n",
        "          D_B Discriminator\n",
        "          '''\n",
        "          prob_fake_b_is_real = discriminator(fake_imgs_b)\n",
        "          lsgan_loss_b = lsgan_loss_generator(prob_fake_b_is_real)\n",
        "\n",
        "          \n",
        "\n",
        "          '''\n",
        "          Optimizing the G_B network E/D \n",
        "          (real_B -> fake_A)\n",
        "          '''\n",
        "\n",
        "          latent_real_b, latent_real_b_II = Encoder(real_B)\n",
        "          fake_imgs_a = Decoder(latent_real_b, real_B)\n",
        "\n",
        "          '''\n",
        "          Optimizing Segmentation network real\n",
        "          '''\n",
        "          # optimizer_seg.zero_grad() # Encoder and seg1 + seg2\n",
        "          pred_mask_b = segmenter(latent_real_b)\n",
        "          pred_mask_b_II=segmenter_II(latent_real_b_II)\n",
        "\n",
        "          '''\n",
        "          D_A discriminator\n",
        "          '''\n",
        "          prob_fake_a_is_real, prob_fake_a_aux_is_real = discriminator_aux(fake_imgs_a)\n",
        "          lsgan_loss_a = lsgan_loss_generator(prob_fake_a_is_real)\n",
        "\n",
        "          '''\n",
        "          Not adding lsgan_loss_a and lsgan_loss_b ?\n",
        "          '''\n",
        "\n",
        "          ##################### GAN Cycle loss\n",
        "\n",
        "          latent_fake_b, latent_fake_b_II = Encoder(fake_imgs_b)\n",
        "          cycle_imgs_a = Decoder(latent_fake_b, fake_imgs_b)\n",
        "\n",
        "          '''\n",
        "          Optimizing Segmentation network fake\n",
        "          '''\n",
        "          pred_mask_fake_b=segmenter(latent_fake_b)\n",
        "          # ce_loss_b, dice_loss_b = task_loss(pred_mask_fake_b, gt_A)\n",
        "          pred_mask_fake_b_II=segmenter_II(latent_fake_b_II)\n",
        "\n",
        "          cycle_consistency_loss_a = LAMBDA_A * cycle_consistency_loss(real_images=real_A, generated_images=cycle_imgs_a)\n",
        "          \n",
        "          cycle_imgs_b = G_A(fake_imgs_a)\n",
        "          cycle_consistency_loss_b = LAMBDA_B * cycle_consistency_loss(real_images=real_B, generated_images=cycle_imgs_b)\n",
        "\n",
        "          \n",
        "          ####################### Total Generator losses (not adding g_loss_a and g_loss_b ?)\n",
        "          '''\n",
        "          final G_A loss \n",
        "          '''\n",
        "          g_loss_a = cycle_consistency_loss_a + cycle_consistency_loss_b + lsgan_loss_b\n",
        "          '''\n",
        "          final E/D loss\n",
        "          '''\n",
        "          g_loss_b = cycle_consistency_loss_b + cycle_consistency_loss_a + lsgan_loss_a\n",
        "\n",
        "          '''\n",
        "          backward\n",
        "          '''\n",
        "          g_loss_a.backward(retain_graph=True)\n",
        "          # optimizer_G_A.step() # Generator\n",
        "          g_loss_b.backward(retain_graph=True)\n",
        "          # optimizer_G_B.step() # Decoder\n",
        "          # optimizer_G.step()\n",
        "\n",
        "          ############### Computing Discriminator losses\n",
        "          ############### Computing D_A loss\n",
        "\n",
        "          optimizer_D_B.zero_grad()\n",
        "          '''\n",
        "          Optimizing the D_B network\n",
        "          '''\n",
        "          \n",
        "          fake_pool_b=fake_imgs_b\n",
        "          fake_pool_b=fake_pool_b.detach()\n",
        "          prob_real_b_is_real = discriminator(real_B)\n",
        "          prob_fake_pool_b_is_real = discriminator(fake_pool_b)\n",
        "\n",
        "          d_loss_b = lsgan_loss_discriminator(prob_real_b_is_real, prob_fake_pool_b_is_real)\n",
        "\n",
        "          '''\n",
        "          backward\n",
        "          '''\n",
        "          d_loss_b.backward(retain_graph=True)\n",
        "          # optimizer_D_B.step()\n",
        "\n",
        "          ############### Computing D_B loss\n",
        "          '''\n",
        "          Optimizing the D_A network\n",
        "          '''\n",
        "\n",
        "          optimizer_D_A.zero_grad()\n",
        "          \n",
        "          fake_pool_a = fake_imgs_a\n",
        "          fake_pool_a=fake_pool_a.detach()\n",
        "          prob_real_a_is_real, prob_real_a_aux = discriminator_aux(real_A)\n",
        "          prob_fake_pool_a_is_real, prob_fake_pool_a_aux_is_real = discriminator_aux(fake_pool_a)\n",
        "\n",
        "          d_loss_a = lsgan_loss_discriminator(\n",
        "              prob_real_a_is_real,\n",
        "              prob_fake_pool_a_is_real,\n",
        "          )\n",
        "\n",
        "          '''\n",
        "          ???? \n",
        "          '''\n",
        "          cycle_imgs_a=cycle_imgs_a.detach()\n",
        "          prob_cycle_a_is_real, prob_cycle_a_aux_is_real = discriminator_aux(cycle_imgs_a)\n",
        "\n",
        "          d_loss_a_aux = lsgan_loss_discriminator(\n",
        "              prob_cycle_a_aux_is_real,\n",
        "              prob_fake_pool_a_aux_is_real,\n",
        "          )\n",
        "\n",
        "          d_loss_a = d_loss_a + d_loss_a_aux\n",
        "\n",
        "          '''\n",
        "          backward\n",
        "          '''\n",
        "          d_loss_a.backward(retain_graph=True)\n",
        "          # optimizer_D_A.step()\n",
        "          \n",
        "          # optimizer_seg.zero_grad() # Encoder and seg1 + seg2\n",
        "\n",
        "  \n",
        "          ce_loss_b, dice_loss_b = task_loss(pred_mask_fake_b, gt_A)\n",
        "          # loss = nn.MSELoss()\n",
        "          # gt_A=gt_A.to(torch.float)\n",
        "          # ce_loss_b = loss(pred_mask_fake_b, gt_A)\n",
        "          \n",
        "          ce_loss_b_II, dice_loss_b_II = task_loss(pred_mask_fake_b_II, gt_A)\n",
        "          l2_loss_b = l2_loss()\n",
        "\n",
        "\n",
        "          '''\n",
        "          Discriminator P\n",
        "          '''          \n",
        "\n",
        "          optimizer_D_P.zero_grad()\n",
        "          optimizer_D_P_II.zero_grad()\n",
        "\n",
        "          prob_pred_mask_b_is_real=discriminator_p(pred_mask_b)#################\n",
        "          lsgan_loss_p=lsgan_loss_generator(prob_pred_mask_b_is_real)\n",
        "\n",
        "          '''\n",
        "          Discriminator P II\n",
        "          '''\n",
        "          prob_pred_mask_b_II_is_real=discriminator_p_II(pred_mask_b_II) ###############\n",
        "          lsgan_loss_p_II=lsgan_loss_generator(prob_pred_mask_b_II_is_real)     \n",
        "          \n",
        "          lsgan_loss_a_aux=lsgan_loss_generator(prob_fake_a_aux_is_real)\n",
        "\n",
        "          seg_loss = ce_loss_b + dice_loss_b + l2_loss_b + 0.1*(ce_loss_b_II + dice_loss_b_II) +0.1*g_loss_b +0.1*lsgan_loss_p+0.01*lsgan_loss_p_II+0.1*lsgan_loss_a_aux\n",
        "          \n",
        "          seg_loss.backward(retain_graph=True)\n",
        "          # optimizer_seg.step()\n",
        "\n",
        "          \n",
        "          # optimizer_G.step()\n",
        "          # optimizer_D_A.step()\n",
        "          # optimizer_D_B.step()\n",
        "\n",
        "          '''\n",
        "          optimizing D_P\n",
        "          '''\n",
        "          # optimizer_D_P.zero_grad()\n",
        "\n",
        "          prob_pred_mask_fake_b_is_real=discriminator_p(pred_mask_fake_b)\n",
        "          \n",
        "          \n",
        "          d_loss_P = lsgan_loss_discriminator(\n",
        "              prob_pred_mask_fake_b_is_real,\n",
        "              prob_pred_mask_b_is_real,\n",
        "          )\n",
        "          d_loss_P.backward(retain_graph=True)\n",
        "          # optimizer_D_P.step()\n",
        "\n",
        "          '''\n",
        "          optimizing D_P_II\n",
        "          '''\n",
        "          \n",
        "\n",
        "          prob_pred_mask_fake_b_II_is_real = discriminator_p_II(pred_mask_fake_b_II)\n",
        "      \n",
        "\n",
        "          d_loss_P_II = lsgan_loss_discriminator(\n",
        "              prob_pred_mask_fake_b_II_is_real,\n",
        "              prob_pred_mask_b_II_is_real\n",
        "          )\n",
        "\n",
        "          d_loss_P_II.backward()\n",
        "\n",
        "          optimizer_D_P_II.step()\n",
        "          optimizer_D_P.step()\n",
        "\n",
        "          optimizer_seg.step()\n",
        "\n",
        "          \n",
        "          optimizer_G.step()\n",
        "          optimizer_D_A.step()\n",
        "          optimizer_D_B.step()\n",
        "\n",
        "          # optimizer_D_P_II\n",
        "\n",
        "\n",
        "          # sys.stdout.write(\n",
        "          #     \"[Epoch %d/%d] [Batch %d/%d] [D_A loss: %f] [D_B loss: %f] [G_A loss: %f] [G_B loss: %f]\\n\"\n",
        "          #     % (\n",
        "          #         epoch,\n",
        "          #         opt[\"n_epochs\"],\n",
        "          #         i,\n",
        "          #         len(dataloader),\n",
        "          #         d_loss_a.item(),\n",
        "          #         d_loss_b.item(),\n",
        "          #         g_loss_a.item(),\n",
        "          #         g_loss_b.item(),\n",
        "          #     )\n",
        "          # )\n",
        "\n",
        "          '''\n",
        "          Putting assertions to make sure gradients flow\n",
        "          '''\n",
        "          for param in list(discriminator.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(G_A.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(discriminator_aux.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(Encoder.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(Decoder.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(segmenter.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(segmenter_II.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(discriminator_p_II.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          for param in list(discriminator_p.parameters()):\n",
        "            assert param.grad is not None\n",
        "\n",
        "          sys.stdout.write(\n",
        "              \"\\r[Epoch %d/%d] [G_A loss: %f] [D_B loss: %f]\"\n",
        "              % (\n",
        "                  epoch,\n",
        "                  opt[\"n_epochs\"],\n",
        "                  g_loss_b.item(),\n",
        "                  d_loss_b.item()\n",
        "              )\n",
        "          )\n",
        "          \n",
        "          if epoch%100==0:\n",
        "            soft=torch.nn.functional.softmax(pred_mask_fake_b,dim=1)\n",
        "            args=torch.argmax(soft.detach().cpu(), dim=1)\n",
        "            args=torch.squeeze(args)\n",
        "            ne=args.clone()\n",
        "            for i in range(65):\n",
        "                for j in range(65):\n",
        "                    ne[i][j]=pred_mask_fake_b[0].permute(1,2,0)[i,j,args[i][j]]\n",
        "\n",
        "            plt.imshow(ne, cmap='gray')\n",
        "            plt.show()\n",
        "          \n",
        "        if epoch%1000==0:\n",
        "          with open('fakeb'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(fake_imgs_b, f)\n",
        "          with open('fakea'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(fake_imgs_a, f)\n",
        "          with open('cyclea'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(cycle_imgs_a, f)\n",
        "          with open('cycleb'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(cycle_imgs_b, f)\n",
        "          with open('pred_mask_fake_b'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(pred_mask_fake_b, f)\n",
        "          with open('pred_mask_fake_b_II'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(pred_mask_fake_b_II, f)\n",
        "          with open('pred_mask_b'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(pred_mask_b, f)\n",
        "          with open('pred_mask_b_II'+str(epoch)+'.pickle', 'wb') as f:\n",
        "            pickle.dump(pred_mask_b_II, f)\n",
        "\n",
        "        G_A.eval()\n",
        "        discriminator.eval()\n",
        "        Encoder.eval()\n",
        "        Decoder.eval()\n",
        "        discriminator_aux.eval()\n",
        "        segmenter.eval()\n",
        "        segmenter_II.eval()\n",
        "        discriminator_p.eval()\n",
        "        discriminator_p_II.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          \n",
        "\n",
        "\n",
        "          fake_imgs_b = G_A(real_A)\n",
        "\n",
        "          '''\n",
        "          Optimizing the G_B network E/D \n",
        "          (real_B -> fake_A)\n",
        "          '''\n",
        "\n",
        "          latent_real_b, latent_real_b_II = Encoder(real_B)\n",
        "          fake_imgs_a = Decoder(latent_real_b, real_B)\n",
        "\n",
        "          '''\n",
        "          Optimizing Segmentation network real\n",
        "          '''\n",
        "          # optimizer_seg.zero_grad() # Encoder and seg1 + seg2\n",
        "          pred_mask_b = segmenter(latent_real_b)\n",
        "          pred_mask_b_II=segmenter_II(latent_real_b_II)\n",
        "\n",
        "  \n",
        "          latent_fake_b, latent_fake_b_II = Encoder(fake_imgs_b)\n",
        "          cycle_imgs_a = Decoder(latent_fake_b, fake_imgs_b)\n",
        "\n",
        "          '''\n",
        "          Optimizing Segmentation network fake\n",
        "          '''\n",
        "          pred_mask_fake_b=segmenter(latent_fake_b)\n",
        "          # ce_loss_b, dice_loss_b = task_loss(pred_mask_fake_b, gt_A)\n",
        "          pred_mask_fake_b_II=segmenter_II(latent_fake_b_II)\n",
        "\n",
        "          cycle_imgs_b = G_A(fake_imgs_a)\n",
        "\n",
        "          if epoch%1000==0:\n",
        "            with open('evalfakeb'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(fake_imgs_b, f)\n",
        "            with open('evalfakea'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(fake_imgs_a, f)\n",
        "            with open('evalcyclea'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(cycle_imgs_a, f)\n",
        "            with open('evalcycleb'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(cycle_imgs_b, f)\n",
        "            with open('evalpred_mask_fake_b'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(pred_mask_fake_b, f)\n",
        "            with open('evalpred_mask_fake_b_II'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(pred_mask_fake_b_II, f)\n",
        "            with open('evalpred_mask_b'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(pred_mask_b, f)\n",
        "            with open('evalpred_mask_b_II'+str(epoch)+'.pickle', 'wb') as f:\n",
        "              pickle.dump(pred_mask_b_II, f)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 65, 65])\n",
            "torch.Size([1, 1, 65, 65])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0432dbc25ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m           \u001b[0;31m##################### GAN Cycle loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m           \u001b[0mlatent_fake_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_fake_b_II\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_imgs_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m           \u001b[0mcycle_imgs_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_fake_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_imgs_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f8a291e05d09>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_res8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_res9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_res10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_res11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-078375b9926f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/padding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   3998\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Padding length too large\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3999\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4000\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_pad_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4001\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4002\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding mode \"{}\"\" doesn\\'t take in value argument'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 10.31 GiB already allocated; 4.81 MiB free; 10.67 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovsri2AaXIcB"
      },
      "source": [
        "# plt.imshow(pred_mask_fake_b[0].detach().cpu().permute(1,2,0)[:,:,0],cmap='gray')\n",
        "# plt.show()\n",
        "vis(pred_mask_fake_b)\n",
        "a=torch.nn.functional.softmax(pred_mask_fake_b)\n",
        "# vis(a)\n",
        "print(a.shape)\n",
        "b=torch.argmax(a, dim=1)\n",
        "print(b.shape)\n",
        "print(1 in b)\n",
        "# print(b[0].shape)\n",
        "# plt.imshow(torch.squeeze(b[0].detach().cpu()),cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_60S6ySvTyqd"
      },
      "source": [
        "with open('fakebfinal.pickle', 'wb') as f:\n",
        "  pickle.dump(fake_imgs_b, f)\n",
        "with open('fakeafinal.pickle', 'wb') as f:\n",
        "  pickle.dump(fake_imgs_a, f)\n",
        "with open('cyclebfinal.pickle', 'wb') as f:\n",
        "  pickle.dump(cycle_imgs_b, f)\n",
        "with open('cycleafinal.pickle', 'wb') as f:\n",
        "  pickle.dump(cycle_imgs_a, f)\n",
        "with open('pred_mask_fake_bfinal.pickle', 'wb') as f:\n",
        "  pickle.dump(pred_mask_fake_b, f)\n",
        "with open('pred_mask_fake_b_IIfinal.pickle', 'wb') as f:\n",
        "  pickle.dump(pred_mask_fake_b_II, f)\n",
        "with open('pred_mask_bfinal.pickle', 'wb') as f:\n",
        "  pickle.dump(pred_mask_b, f)\n",
        "with open('pred_mask_b_IIfinal.pickle', 'wb') as f:\n",
        "  pickle.dump(pred_mask_b_II, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsHaUXTK0lxP"
      },
      "source": [
        "# vis(fake_imgs_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of2afO6AyDsV"
      },
      "source": [
        "# '''\n",
        "# Visualizing generator outputs\n",
        "# '''\n",
        "# plt.imshow(cycle_imgs_a.detach().cpu().squeeze(),cmap='gray')\n",
        "# plt.show()\n",
        "# plt.imshow(fake_imgs_b.detach().cpu().squeeze(),cmap='gray')\n",
        "# plt.show()\n",
        "# plt.imshow(real_A.detach().cpu().squeeze(),cmap='gray')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}